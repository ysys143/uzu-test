#!/usr/bin/env python3
"""
ê° ì—”ì§„ì˜ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë°©ì‹ ë””ë²„ê·¸
"""

import subprocess
import time
import requests
import json
import os

def test_pytorch_prompt():
    """PyTorch í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í™•ì¸"""
    print("ğŸ”¥ PyTorch í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬:")
    
    # HuggingFace tokenizers ë³‘ë ¬ì²˜ë¦¬ ê²½ê³  ì–µì œ
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    tokenizer = AutoTokenizer.from_pretrained('./models/gemma-3-1b-it')
    model = AutoModelForCausalLM.from_pretrained(
        './models/gemma-3-1b-it',
        torch_dtype=torch.float16,
        device_map="mps"
    )
    
    prompt = "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤."
    
    # í† í¬ë‚˜ì´ì € ì ìš© ì „í›„ í™•ì¸
    print(f"  ì›ë³¸ í”„ë¡¬í”„íŠ¸: '{prompt}'")
    
    # Chat templateì´ ìˆëŠ”ì§€ í™•ì¸
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
        print(f"  Chat template: {tokenizer.chat_template}")
        # Messages í˜•ì‹ìœ¼ë¡œ ì ìš©
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)
        print(f"  Chat template ì ìš© í›„: '{formatted_prompt}'")
    else:
        print("  Chat template ì—†ìŒ")
        formatted_prompt = prompt
    
    # í† í°í™” í™•ì¸
    tokens = tokenizer.encode(formatted_prompt, return_tensors="pt")
    print(f"  í† í° ìˆ˜: {len(tokens[0])}")
    print(f"  í† í°ë“¤: {tokens[0].tolist()[:20]}...")  # ì²˜ìŒ 20ê°œë§Œ
    
    # í† í°ì„ ë‹¤ì‹œ ë””ì½”ë”©í•´ì„œ í™•ì¸
    decoded = tokenizer.decode(tokens[0], skip_special_tokens=False)
    print(f"  ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: '{decoded}'")

def test_ollama_prompt():
    """Ollama í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í™•ì¸"""
    print("\nğŸ¦™ Ollama í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬:")
    
    prompt = "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤."
    print(f"  ì›ë³¸ í”„ë¡¬í”„íŠ¸: '{prompt}'")
    
    # Ollama ëª…ë ¹ì–´ë¡œ ì „ì†¡ë˜ëŠ” ì‹¤ì œ í˜•íƒœ
    cmd = ['ollama', 'run', 'gemma2:2b', prompt]
    print(f"  Ollama ëª…ë ¹ì–´: {' '.join(cmd)}")
    
    # Modelfileì—ì„œ template í™•ì¸
    print("  Modelfile template:")
    with open('models/Modelfile', 'r') as f:
        content = f.read()
        print(f"  {content}")

def test_uzu_prompt():
    """Uzu í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í™•ì¸"""
    print("\nâš¡ Uzu í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬:")
    
    prompt = "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤."
    print(f"  ì›ë³¸ í”„ë¡¬í”„íŠ¸: '{prompt}'")
    
    # Uzu API ìš”ì²­ í˜•íƒœ
    payload = {
        "model": "gemma-3-1b-it-uzu",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 50,
        "temperature": 0.7
    }
    print(f"  API ìš”ì²­ í˜•íƒœ:")
    print(f"  {json.dumps(payload, ensure_ascii=False, indent=2)}")

def test_llamacpp_prompt():
    """llama.cpp í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ í™•ì¸"""
    print("\nğŸ¦™ llama.cpp í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬:")
    
    prompt = "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤."
    print(f"  ì›ë³¸ í”„ë¡¬í”„íŠ¸: '{prompt}'")
    
    # llama.cpp ëª…ë ¹ì–´
    cmd = [
        'llama-cli',
        '-m', './models/gemma-3-1b-it-gguf-llama/model.gguf',
        '-p', prompt,
        '-n', '10',  # ì§§ê²Œ í…ŒìŠ¤íŠ¸
        '--temp', '0.7',
        '-ngl', '99',
        '--no-display-prompt',
        '-no-cnv'
    ]
    print(f"  llama.cpp ëª…ë ¹ì–´: {' '.join(cmd)}")

if __name__ == "__main__":
    print("ğŸ” í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë°©ì‹ ë””ë²„ê·¸")
    print("=" * 50)
    
    # PyTorch í…ŒìŠ¤íŠ¸ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
    try:
        test_pytorch_prompt()
    except Exception as e:
        print(f"  PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ë‹¤ë¥¸ ì—”ì§„ë“¤ í”„ë¡¬í”„íŠ¸ í˜•íƒœ í™•ì¸
    test_ollama_prompt()
    test_uzu_prompt()
    test_llamacpp_prompt()
    
    print("\nâœ… ë””ë²„ê·¸ ì™„ë£Œ!") 