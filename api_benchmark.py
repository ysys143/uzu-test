#!/usr/bin/env python3
"""
API ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ ëŸ¬ë„ˆ
ëª¨ë“  ì—”ì§„ì„ ì„œë²„ í˜•íƒœë¡œ ì‹¤í–‰í•˜ì—¬ ìˆœìˆ˜í•œ ì¶”ë¡  ì„±ëŠ¥ë§Œ ì¸¡ì •
"""

import time
import requests
import json
import statistics
from datetime import datetime
from typing import Dict, List, Optional
import os
import logging
from server_manager import ServerManager
from benchmark_prompts import get_all_prompts, SYSTEM_PROMPT

class APIBenchmarkRunner:
    def __init__(self, num_runs: int = None, config_file: str = "benchmark_config.json", quick_test: bool = False):
        # ì„¤ì • íŒŒì¼ ë¡œë”©
        self.config = self._load_config(config_file)
        
        # í™˜ê²½ë³€ìˆ˜ì™€ ì„¤ì • íŒŒì¼ì—ì„œ ë§¤ê°œë³€ìˆ˜ ë¡œë”©
        self.num_runs = num_runs or int(os.getenv('BENCHMARK_NUM_RUNS', self.config['benchmark']['num_runs']))
        self.max_tokens = int(os.getenv('BENCHMARK_MAX_TOKENS', self.config['benchmark']['max_tokens']))
        self.temperature = float(os.getenv('BENCHMARK_TEMPERATURE', self.config['benchmark']['temperature']))
        self.timeout_seconds = int(os.getenv('BENCHMARK_TIMEOUT', self.config['benchmark']['timeout_seconds']))
        self.quick_test = quick_test or os.getenv('BENCHMARK_QUICK_TEST', '').lower() in ('true', '1', 'yes')
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('report', exist_ok=True)
        os.makedirs('output', exist_ok=True)
        os.makedirs(self.config['logging']['directory'], exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        logging_config = self.config['logging']
        self.log_filename = f"{logging_config['directory']}/api_benchmark_detailed_{self.timestamp}.log"
        
        # ë¡œê¹… ì„¤ì •
        handlers = [logging.FileHandler(self.log_filename, encoding='utf-8')]
        if logging_config.get('console_output', True):
            handlers.append(logging.StreamHandler())
            
        logging.basicConfig(
            level=getattr(logging, logging_config.get('level', 'INFO')),
            format='%(asctime)s - %(message)s',
            handlers=handlers
        )
        self.logger = logging.getLogger(__name__)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        system_prompt_override = os.getenv('BENCHMARK_SYSTEM_PROMPT') or self.config['benchmark'].get('system_prompt_override')
        self.system_prompt = system_prompt_override if system_prompt_override else SYSTEM_PROMPT
        
        # í”„ë¡¬í”„íŠ¸ ì„ íƒ
        all_prompts = get_all_prompts()
        if self.quick_test:
            self.test_prompts = [all_prompts[0]]  # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ë§Œ
        else:
            self.test_prompts = [
                all_prompts[0], all_prompts[1], all_prompts[4], all_prompts[8],
                all_prompts[18], all_prompts[22], all_prompts[25],
                all_prompts[34], all_prompts[40], all_prompts[45]
            ]
        
        # ì„œë²„ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.server_manager = ServerManager(self.config)
        self.results = {}
        
    def _load_config(self, config_file: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë”©"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë”© ì„±ê³µ: {config_file}")
            return config
        except FileNotFoundError:
            print(f"âš ï¸  ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self._get_default_config()
        except json.JSONDecodeError as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            "benchmark": {"max_tokens": 500, "temperature": 0.3, "num_runs": 10, "timeout_seconds": 120},
            "servers": {
                "pytorch": {"enabled": True, "port": 8001, "model_path": "./models/gemma-3-1b-it", "startup_timeout": 60, "api_endpoint": "/chat/completions"},
                "ollama": {"enabled": True, "port": 11434, "model_name": "gemma-3-1b-it-bench", "startup_timeout": 30, "api_endpoint": "/api/generate"},
                "llamacpp": {"enabled": True, "port": 8002, "model_path": "./models/gemma-3-1b-it-gguf-llama/model.gguf", "startup_timeout": 30, "api_endpoint": "/completion"},
                "uzu": {"enabled": True, "port": 8000, "model_path": "./models/gemma-3-1b-it-uzu", "startup_timeout": 60, "api_endpoint": "/chat/completions"}
            },
            "logging": {"directory": "logging", "level": "INFO", "console_output": True}
        }
    
    def test_server_api(self, server_name: str, prompt: str) -> Optional[Dict]:
        """ê°œë³„ ì„œë²„ API í…ŒìŠ¤íŠ¸"""
        server_config = self.config['servers'][server_name]
        base_url = self.server_manager.get_server_url(server_name)
        
        if not base_url:
            self.logger.error(f"{server_name} ì„œë²„ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        self.logger.info(f"[{server_name}] API í…ŒìŠ¤íŠ¸ ì‹œì‘")
        self.logger.info(f"[{server_name}] í”„ë¡¬í”„íŠ¸: {prompt}")
        
        try:
            if server_name == "pytorch" or server_name == "uzu":
                return self._test_openai_api(server_name, base_url, server_config, prompt)
            elif server_name == "ollama":
                return self._test_ollama_api(server_name, base_url, server_config, prompt)
            elif server_name == "llamacpp":
                return self._test_llamacpp_api(server_name, base_url, server_config, prompt)
            else:
                self.logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì„œë²„ íƒ€ì…: {server_name}")
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì„œë²„ íƒ€ì…: {server_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"{server_name} API í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}", exc_info=True)
            print(f"âŒ {server_name} API í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return None
    
    def _test_openai_api(self, server_name: str, base_url: str, config: Dict, prompt: str) -> Dict:
        """OpenAI í˜¸í™˜ API í…ŒìŠ¤íŠ¸ (PyTorch, Uzu)"""
        payload = {
            "model": os.path.basename(config.get('model_path', server_name)),
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        }
        
        start_time = time.time()
        response = requests.post(
            f"{base_url}{config['api_endpoint']}",
            json=payload,
            timeout=self.timeout_seconds
        )
        inference_time = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            response_text = data['choices'][0]['message']['content']
            
            # í† í° ìˆ˜ ê³„ì‚° (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            tokens_used = data.get('usage', {}).get('completion_tokens', len(response_text.split()))
            tps = tokens_used / inference_time if inference_time > 0 else 0
            
            # ë¡œê·¸ ê¸°ë¡
            self.logger.info(f"[{server_name}] ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ")
            self.logger.info(f"[{server_name}] ìƒì„± í† í°: {tokens_used}ê°œ")
            self.logger.info(f"[{server_name}] TPS: {tps:.2f}")
            self.logger.info(f"[{server_name}] ì‘ë‹µ ê¸¸ì´: {len(response_text.split())}ë‹¨ì–´, {len(response_text)}ì")
            self.logger.info(f"[{server_name}] ì‘ë‹µ ë‚´ìš©: {response_text}")
            self.logger.info(f"[{server_name}] " + "-" * 60)
            
            return {
                'success': True,
                'response': response_text,
                'inference_time': inference_time,
                'tokens_generated': tokens_used,
                'tps': tps
            }
        else:
            self.logger.error(f"{server_name} API ì˜¤ë¥˜: HTTP {response.status_code}, ì‘ë‹µ: {response.text[:200]}")
            print(f"âŒ {server_name} API ì˜¤ë¥˜: HTTP {response.status_code}")
            return {'success': False, 'error': f"HTTP {response.status_code}"}
    
    def _test_ollama_api(self, server_name: str, base_url: str, config: Dict, prompt: str) -> Dict:
        """Ollama API í…ŒìŠ¤íŠ¸"""
        full_prompt = f"{self.system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
        
        payload = {
            "model": config['model_name'],
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "num_predict": self.max_tokens,
                "temperature": self.temperature
            }
        }
        
        start_time = time.time()
        response = requests.post(
            f"{base_url}{config['api_endpoint']}",
            json=payload,
            timeout=self.timeout_seconds
        )
        inference_time = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            response_text = data.get('response', '')
            
            # OllamaëŠ” í† í° ì†ë„ë¥¼ ì§ì ‘ ì œê³µ
            eval_count = data.get('eval_count', len(response_text.split()))
            tps = eval_count / inference_time if inference_time > 0 else 0
            
            # ë¡œê·¸ ê¸°ë¡
            self.logger.info(f"[{server_name}] ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ")
            self.logger.info(f"[{server_name}] ìƒì„± í† í°: {eval_count}ê°œ")
            self.logger.info(f"[{server_name}] TPS: {tps:.2f}")
            self.logger.info(f"[{server_name}] ì‘ë‹µ ê¸¸ì´: {len(response_text.split())}ë‹¨ì–´, {len(response_text)}ì")
            self.logger.info(f"[{server_name}] ì‘ë‹µ ë‚´ìš©: {response_text}")
            self.logger.info(f"[{server_name}] " + "-" * 60)
            
            return {
                'success': True,
                'response': response_text,
                'inference_time': inference_time,
                'tokens_generated': eval_count,
                'tps': tps
            }
        else:
            self.logger.error(f"{server_name} API ì˜¤ë¥˜: HTTP {response.status_code}, ì‘ë‹µ: {response.text[:200]}")
            print(f"âŒ {server_name} API ì˜¤ë¥˜: HTTP {response.status_code}")
            return {'success': False, 'error': f"HTTP {response.status_code}"}
    
    def _test_llamacpp_api(self, server_name: str, base_url: str, config: Dict, prompt: str) -> Dict:
        """llama.cpp ì„œë²„ API í…ŒìŠ¤íŠ¸"""
        full_prompt = f"{self.system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
        
        payload = {
            "prompt": full_prompt,
            "n_predict": self.max_tokens,
            "temperature": self.temperature,
            "stream": False
        }
        
        start_time = time.time()
        response = requests.post(
            f"{base_url}{config['api_endpoint']}",
            json=payload,
            timeout=self.timeout_seconds
        )
        inference_time = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            response_text = data.get('content', '')
            
            # í† í° ì •ë³´ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            tokens_predicted = data.get('tokens_predicted', len(response_text.split()))
            tps = tokens_predicted / inference_time if inference_time > 0 else 0
            
            # ë¡œê·¸ ê¸°ë¡
            self.logger.info(f"[{server_name}] ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ")
            self.logger.info(f"[{server_name}] ìƒì„± í† í°: {tokens_predicted}ê°œ")
            self.logger.info(f"[{server_name}] TPS: {tps:.2f}")
            self.logger.info(f"[{server_name}] ì‘ë‹µ ê¸¸ì´: {len(response_text.split())}ë‹¨ì–´, {len(response_text)}ì")
            self.logger.info(f"[{server_name}] ì‘ë‹µ ë‚´ìš©: {response_text}")
            self.logger.info(f"[{server_name}] " + "-" * 60)
            
            return {
                'success': True,
                'response': response_text,
                'inference_time': inference_time,
                'tokens_generated': tokens_predicted,
                'tps': tps
            }
        else:
            self.logger.error(f"{server_name} API ì˜¤ë¥˜: HTTP {response.status_code}, ì‘ë‹µ: {response.text[:200]}")
            print(f"âŒ {server_name} API ì˜¤ë¥˜: HTTP {response.status_code}")
            return {'success': False, 'error': f"HTTP {response.status_code}"}
    
    def run_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ API ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        print(f"ğŸ“Š ì„¤ì •: {self.num_runs}íšŒ ë°˜ë³µ, {len(self.test_prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
        print(f"âš¡ ëª¨ë“œ: {'ë¹ ë¥¸ í…ŒìŠ¤íŠ¸' if self.quick_test else 'ì •ì‹ ë²¤ì¹˜ë§ˆí¬'}")
        print(f"ğŸ“ ìƒì„¸ ë¡œê·¸: {self.log_filename}")
        
        # ë¡œê·¸ì— ë²¤ì¹˜ë§ˆí¬ ì •ë³´ ê¸°ë¡
        self.logger.info("ğŸš€ API ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        self.logger.info(f"ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}íšŒ")
        self.logger.info(f"í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(self.test_prompts)}ê°œ")
        self.logger.info(f"ìµœëŒ€ í† í° ìˆ˜: {self.max_tokens}")
        self.logger.info(f"ì˜¨ë„ ì„¤ì •: {self.temperature}")
        self.logger.info(f"ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {self.quick_test}")
        
        # ì„œë²„ ì‹œì‘
        started_servers = self.server_manager.start_all_servers()
        if not started_servers:
            self.logger.error("ì‹œì‘ëœ ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("âŒ ì‹œì‘ëœ ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ê° ì„œë²„ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            for server_name in started_servers:
                print(f"\nğŸ”¥ {server_name} ì„œë²„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
                self.logger.info(f"=== {server_name} ì„œë²„ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
                server_results = []
                
                for run_idx in range(self.num_runs):
                    print(f"  ì‹¤í–‰ {run_idx + 1}/{self.num_runs}...")
                    self.logger.info(f"[{server_name}] ì‹¤í–‰ {run_idx + 1}/{self.num_runs} ì‹œì‘")
                    run_data = []
                    
                    for prompt_idx, prompt in enumerate(self.test_prompts):
                        result = self.test_server_api(server_name, prompt)
                        if result and result.get('success'):
                            run_data.append({
                                'prompt_idx': prompt_idx,
                                'prompt': prompt,
                                'response': result['response'],
                                'inference_time': result['inference_time'],
                                'tokens_generated': result['tokens_generated'],
                                'tps': result['tps']
                            })
                            print(f"    í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}: {result['tps']:.2f} TPS")
                        else:
                            print(f"    í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}: ì‹¤íŒ¨")
                    
                    if run_data:
                        avg_tps = sum(r['tps'] for r in run_data) / len(run_data)
                        server_results.append({
                            'run_index': run_idx,
                            'avg_tps': avg_tps,
                            'tests': run_data
                        })
                        print(f"    ì‹¤í–‰ {run_idx + 1} í‰ê·  TPS: {avg_tps:.2f}")
                        self.logger.info(f"[{server_name}] ì‹¤í–‰ {run_idx + 1} í‰ê·  TPS: {avg_tps:.2f}")
                    else:
                        self.logger.warning(f"[{server_name}] ì‹¤í–‰ {run_idx + 1}: ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì‹¤íŒ¨")
                
                # ì„œë²„ë³„ í†µê³„ ê³„ì‚°
                if server_results:
                    all_tps = [test['tps'] for run in server_results for test in run['tests']]
                    all_times = [test['inference_time'] for run in server_results for test in run['tests']]
                    
                    self.results[server_name] = {
                        'engine': f"{server_name.title()} Server",
                        'num_runs': self.num_runs,
                        'statistics': {
                            'tps': self._calculate_statistics(all_tps),
                            'inference_time': self._calculate_statistics(all_times),
                        },
                        'all_runs': server_results
                    }
                    
                    avg_tps = self.results[server_name]['statistics']['tps']['mean']
                    print(f"âœ… {server_name} ì™„ë£Œ - ì „ì²´ í‰ê·  TPS: {avg_tps:.2f}")
                    self.logger.info(f"[{server_name}] í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì „ì²´ í‰ê·  TPS: {avg_tps:.2f}")
                else:
                    print(f"âŒ {server_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    self.logger.error(f"[{server_name}] í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        finally:
            # ì„œë²„ ì¢…ë£Œ
            self.server_manager.stop_all_servers()
        
        # ê²°ê³¼ ìƒì„±
        self._generate_report()
    
    def _calculate_statistics(self, values: List[float]) -> Dict:
        """í†µê³„ ê³„ì‚°"""
        if not values:
            return {'mean': 0, 'median': 0, 'min': 0, 'max': 0, 'std': 0, 'count': 0}
        
        return {
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'min': min(values),
            'max': max(values),
            'std': statistics.stdev(values) if len(values) > 1 else 0,
            'count': len(values)
        }
    
    def _generate_report(self):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.results:
            print("âŒ ìƒì„±í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        timestamp = datetime.now().isoformat()
        
        # ì½˜ì†” ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ¯ API ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ê²°ê³¼")
        print(f"ì‹¤í–‰ ì‹œê°„: {timestamp}")
        print("="*80)
        
        # ì„±ëŠ¥ í…Œì´ë¸”
        baseline_tps = list(self.results.values())[0]['statistics']['tps']['mean']
        
        print(f"{'ì„œë²„':<15} {'í‰ê· TPS':<10} {'TPSë²”ìœ„':<15} {'í‘œì¤€í¸ì°¨':<10} {'ìƒëŒ€ì„±ëŠ¥':<10}")
        print("-" * 75)
        
        for server_name, data in self.results.items():
            tps_stats = data['statistics']['tps']
            avg_tps = tps_stats['mean']
            min_tps = tps_stats['min']
            max_tps = tps_stats['max']
            std_tps = tps_stats['std']
            relative = avg_tps / baseline_tps if baseline_tps > 0 else 0
            
            tps_range = f"{min_tps:.1f}-{max_tps:.1f}"
            print(f"{data['engine']:<15} {avg_tps:<10.2f} {tps_range:<15} {std_tps:<10.2f} {relative:<10.1f}x")
        
        # íŒŒì¼ ì €ì¥
        quick_suffix = "_quick" if self.quick_test else ""
        
        # JSON ë°ì´í„° ì €ì¥
        json_file = f'output/api_benchmark_results_{self.num_runs}runs{quick_suffix}_{self.timestamp}.json'
        detailed_results = {
            'benchmark_info': {
                'timestamp': timestamp,
                'mode': 'api_based',
                'num_runs': self.num_runs,
                'num_prompts': len(self.test_prompts),
                'max_tokens': self.max_tokens,
                'temperature': self.temperature,
                'quick_test': self.quick_test
            },
            'results': self.results
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # Markdown ë¦¬í¬íŠ¸ ì €ì¥
        md_file = f'report/api_benchmark_report_{self.num_runs}runs{quick_suffix}_{self.timestamp}.md'
        md_content = self._generate_markdown_report(timestamp, baseline_tps)
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        print(f"\nğŸ“Š ìƒì„¸ ê²°ê³¼: {json_file}")
        print(f"ğŸ“‹ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸: {md_file}")
        print(f"ğŸ“ ìƒì„¸ ë¡œê·¸: {self.log_filename}")
        
        self.logger.info("ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        self.logger.info(f"JSON ê²°ê³¼ íŒŒì¼: {json_file}")
        self.logger.info(f"Markdown ë¦¬í¬íŠ¸: {md_file}")
    
    def _generate_markdown_report(self, timestamp: str, baseline_tps: float) -> str:
        """TPSì™€ Latencyë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì¢…í•©ì ì¸ Markdown ë¦¬í¬íŠ¸ ìƒì„±"""
        md_content = f"""# API ê¸°ë°˜ ì¶”ë¡  ì—”ì§„ ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ êµ¬ì„±
- **ì‹¤í–‰ ì‹œê°„**: {timestamp}
- **ëª¨ë“œ**: {'ë¹ ë¥¸ í…ŒìŠ¤íŠ¸' if self.quick_test else 'ì •ì‹ ë²¤ì¹˜ë§ˆí¬'}
- **ë°˜ë³µ íšŸìˆ˜**: {self.num_runs}íšŒ
- **í”„ë¡¬í”„íŠ¸ ìˆ˜**: {len(self.test_prompts)}ê°œ
- **ìµœëŒ€ í† í°**: {self.max_tokens}ê°œ
- **ì˜¨ë„ ì„¤ì •**: {self.temperature} (ì¼ê´€ëœ ì‘ë‹µ)
- **ì´ ì‹¤í–‰ íšŸìˆ˜**: {len(self.test_prompts) * self.num_runs * len(self.results)}íšŒ

## ğŸš€ ì„±ëŠ¥ ìš”ì•½

### ì²˜ë¦¬ëŸ‰ ì„±ëŠ¥ (TPS)

| ìˆœìœ„ | ì„œë²„ | í‰ê·  TPS | TPS ë²”ìœ„ | í‘œì¤€í¸ì°¨ | ìƒëŒ€ ì„±ëŠ¥ |
|------|------|----------|----------|----------|----------|
"""
        
        # TPS ê¸°ì¤€ ì •ë ¬
        sorted_servers_tps = sorted(
            [(server, data) for server, data in self.results.items()], 
            key=lambda x: x[1]['statistics']['tps']['mean'], 
            reverse=True
        )
        
        for rank, (server_name, data) in enumerate(sorted_servers_tps, 1):
            tps_stats = data['statistics']['tps']
            avg_tps = tps_stats['mean']
            min_tps = tps_stats['min']
            max_tps = tps_stats['max']
            std_tps = tps_stats['std']
            relative = avg_tps / baseline_tps if baseline_tps > 0 else 0
            
            tps_range = f"{min_tps:.1f}-{max_tps:.1f}"
            md_content += f"| {rank}ìœ„ | **{data['engine']}** | {avg_tps:.2f} | {tps_range} | {std_tps:.2f} | **{relative:.1f}x** |\n"
        
        # Latency ê¸°ì¤€ ì„±ëŠ¥ í‘œ
        md_content += "\n### ì‘ë‹µ ì†ë„ ì„±ëŠ¥ (Latency)\n\n"
        md_content += "| ìˆœìœ„ | ì„œë²„ | í‰ê·  Latency (ì´ˆ) | Latency ë²”ìœ„ | í‘œì¤€í¸ì°¨ | ì¼ê´€ì„± |\n"
        md_content += "|------|------|-------------------|--------------|----------|--------|\n"
        
        # Latency ê¸°ì¤€ ì •ë ¬ (ë‚®ì€ ìˆœ)
        sorted_servers_latency = sorted(
            [(server, data) for server, data in self.results.items()], 
            key=lambda x: x[1]['statistics']['inference_time']['mean']
        )
        
        for rank, (server_name, data) in enumerate(sorted_servers_latency, 1):
            latency_stats = data['statistics']['inference_time']
            avg_latency = latency_stats['mean']
            min_latency = latency_stats['min']
            max_latency = latency_stats['max']
            std_latency = latency_stats['std']
            
            latency_range = f"{min_latency:.2f}-{max_latency:.2f}"
            consistency = "ë†’ìŒ" if std_latency < avg_latency * 0.2 else "ë³´í†µ" if std_latency < avg_latency * 0.5 else "ë‚®ìŒ"
            
            md_content += f"| {rank}ìœ„ | **{data['engine']}** | {avg_latency:.3f} | {latency_range} | {std_latency:.3f} | {consistency} |\n"
        
        # ìƒì„¸ í†µê³„ ì¶”ê°€
        md_content += "\n## ğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ í†µê³„\n\n"
        
        for server_name, data in self.results.items():
            engine_name = data['engine']
            tps_stats = data['statistics']['tps']
            latency_stats = data['statistics']['inference_time']
            
            md_content += f"### {engine_name}\n\n"
            
            # ì–‘ìª½ ì§€í‘œë¥¼ í…Œì´ë¸”ë¡œ ë¹„êµ
            md_content += "| ì§€í‘œ | í‰ê·  | ì¤‘ê°„ê°’ | ìµœì†Œê°’ | ìµœëŒ€ê°’ | í‘œì¤€í¸ì°¨ |\n"
            md_content += "|------|------|--------|--------|--------|---------|\n"
            md_content += f"| **TPS** | {tps_stats['mean']:.2f} | {tps_stats['median']:.2f} | {tps_stats['min']:.2f} | {tps_stats['max']:.2f} | {tps_stats['std']:.2f} |\n"
            md_content += f"| **Latency (ì´ˆ)** | {latency_stats['mean']:.3f} | {latency_stats['median']:.3f} | {latency_stats['min']:.3f} | {latency_stats['max']:.3f} | {latency_stats['std']:.3f} |\n\n"
        
        # ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜ ì¶”ê°€
        md_content += "## ğŸ” ì„±ëŠ¥ ë¶„ì„\n\n"
        
        # ìµœê³  ì„±ëŠ¥ ì„œë²„ ì‹ë³„
        best_tps_server = sorted_servers_tps[0][0]
        best_latency_server = sorted_servers_latency[0][0]
        
        best_tps_name = self.results[best_tps_server]['engine']
        best_latency_name = self.results[best_latency_server]['engine']
        
        md_content += f"### ì„±ëŠ¥ ìš°ìˆ˜ ì„œë²„\n\n"
        md_content += f"- **ìµœê³  ì²˜ë¦¬ëŸ‰**: {best_tps_name} ({self.results[best_tps_server]['statistics']['tps']['mean']:.2f} TPS)\n"
        md_content += f"- **ìµœë‹¨ ì‘ë‹µì‹œê°„**: {best_latency_name} ({self.results[best_latency_server]['statistics']['inference_time']['mean']:.3f}ì´ˆ)\n\n"
        
        # ì„±ëŠ¥ ì¼ê´€ì„± ë¶„ì„
        md_content += "### ì„±ëŠ¥ ì•ˆì •ì„± ë¶„ì„\n\n"
        stability_analysis = []
        for server_name, data in self.results.items():
            engine_name = data['engine']
            tps_cv = data['statistics']['tps']['std'] / data['statistics']['tps']['mean'] if data['statistics']['tps']['mean'] > 0 else 0
            latency_cv = data['statistics']['inference_time']['std'] / data['statistics']['inference_time']['mean'] if data['statistics']['inference_time']['mean'] > 0 else 0
            stability_analysis.append((engine_name, tps_cv, latency_cv))
        
        # CV(ë³€ë™ê³„ìˆ˜) ê¸°ì¤€ ì •ë ¬
        stability_analysis.sort(key=lambda x: (x[1] + x[2]) / 2)
        
        for rank, (engine_name, tps_cv, latency_cv) in enumerate(stability_analysis, 1):
            stability_rating = "ë†’ìŒ" if (tps_cv + latency_cv) / 2 < 0.1 else "ë³´í†µ" if (tps_cv + latency_cv) / 2 < 0.3 else "ë‚®ìŒ"
            md_content += f"{rank}. **{engine_name}**: TPS ë³€ë™ì„± {tps_cv:.1%}, Latency ë³€ë™ì„± {latency_cv:.1%} (ì•ˆì •ì„±: {stability_rating})\n"
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ê¶Œì¥ì‚¬í•­
        md_content += "\n## ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ê¶Œì¥ ì„œë²„\n\n"
        
        md_content += "### ì²˜ë¦¬ëŸ‰ ìš°ì„  (API ì„œë¹„ìŠ¤)\n"
        for rank, (server, data) in enumerate(sorted_servers_tps[:3], 1):
            engine_name = data['engine']
            avg_tps = data['statistics']['tps']['mean']
            avg_latency = data['statistics']['inference_time']['mean']
            md_content += f"{rank}. **{engine_name}**: {avg_tps:.2f} TPS, {avg_latency:.1f}ì´ˆ ì§€ì—°\n"
        
        md_content += "\n### ì‘ë‹µì†ë„ ìš°ì„  (ì‹¤ì‹œê°„ API)\n"
        for rank, (server, data) in enumerate(sorted_servers_latency[:3], 1):
            engine_name = data['engine']
            avg_tps = data['statistics']['tps']['mean']
            avg_latency = data['statistics']['inference_time']['mean']
            md_content += f"{rank}. **{engine_name}**: {avg_latency:.3f}ì´ˆ ì§€ì—°, {avg_tps:.1f} TPS\n"
        
        md_content += "\n### ì•ˆì •ì„± ìš°ì„  (í”„ë¡œë•ì…˜ API)\n"
        for rank, (engine_name, tps_cv, latency_cv) in enumerate(stability_analysis[:3], 1):
            overall_stability = (tps_cv + latency_cv) / 2
            md_content += f"{rank}. **{engine_name}**: ì „ì²´ ë³€ë™ì„± {overall_stability:.1%}\n"
        
        # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
        md_content += "\n## ğŸ“‹ ì¢…í•© ê²°ë¡ \n\n"
        
        # ì¢…í•© ìš°ìˆ˜ ì„œë²„ ì„ ì • (TPSì™€ Latency ê°€ì¤‘ í‰ê· )
        overall_scores = []
        for server_name, data in self.results.items():
            engine_name = data['engine']
            tps_score = data['statistics']['tps']['mean'] / max(d['statistics']['tps']['mean'] for d in self.results.values())
            latency_score = min(d['statistics']['inference_time']['mean'] for d in self.results.values()) / data['statistics']['inference_time']['mean']
            overall_score = (tps_score + latency_score) / 2  # ê· ë“± ê°€ì¤‘ì¹˜
            overall_scores.append((engine_name, overall_score, tps_score, latency_score))
        
        overall_scores.sort(key=lambda x: x[1], reverse=True)
        
        md_content += "**ì¢…í•© ì„±ëŠ¥ ìˆœìœ„** (TPSì™€ Latency ê· ë“± ê°€ì¤‘ì¹˜)\n\n"
        for rank, (engine_name, overall_score, tps_score, latency_score) in enumerate(overall_scores, 1):
            md_content += f"{rank}. **{engine_name}**: ì¢…í•© ì ìˆ˜ {overall_score:.3f} (ì²˜ë¦¬ëŸ‰: {tps_score:.3f}, ì‘ë‹µì†ë„: {latency_score:.3f})\n"
        
        quick_suffix = "_quick" if self.quick_test else ""
        md_content += f"\n---\n\n"
        md_content += f"**ë°ì´í„° íŒŒì¼**\n"
        md_content += f"- JSON ë°ì´í„°: `output/api_benchmark_results_{self.num_runs}runs{quick_suffix}_{self.timestamp}.json`\n"
        md_content += f"- ìƒì„¸ ë¡œê·¸: `{self.log_filename}`\n\n"
        md_content += f"*ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°„: {timestamp}*\n"
        
        return md_content

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    num_runs = None
    quick_test = False
    
    args = sys.argv[1:]
    for arg in args:
        if arg.lower() in ('quick', 'q', '--quick', '-q'):
            quick_test = True
        else:
            try:
                num_runs = int(arg)
                if num_runs < 1:
                    raise ValueError("ì‹¤í–‰ íšŸìˆ˜ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            except ValueError:
                print(f"ì˜¤ë¥˜: ì˜ëª»ëœ ì¸ì '{arg}'")
                print("ì‚¬ìš©ë²•: python api_benchmark.py [ì‹¤í–‰íšŸìˆ˜] [quick]")
                sys.exit(1)
    
    runner = APIBenchmarkRunner(num_runs=num_runs, quick_test=quick_test)
    runner.run_benchmark() 