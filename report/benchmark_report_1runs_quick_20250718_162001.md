# Uzu AI 추론 엔진 종합 성능 분석 리포트

## 📊 시스템 정보
- **모델**: MacBook Pro (Mac15,10)
- **프로세서**: Apple M3 Max
- **CPU 코어**: 14 (10 performance and 4 efficiency)
- **메모리**: 36 GB
- **운영체제**: macOS 15.5 (24F74)
- **Python**: 3.12.3

## ⚙️ 벤치마크 구성
- **실행 시간**: 2025-07-18T16:20:46.051243
- **반복 횟수**: 1회
- **테스트 프롬프트**: 1개
- **최대 토큰**: 500개
- **온도 설정**: 0.3 (일관된 응답)
- **총 실행 횟수**: 3회

## 🚀 성능 요약

### 처리량 성능 (TPS)

| 순위 | 엔진 | 평균 TPS | TPS 범위 | 표준편차 | 상대 성능 |
|------|------|----------|----------|----------|----------|
| 1위 | **Ollama (GGUF)** | 132.07 | 132.1-132.1 | 0.00 | **17.8x** |
| 2위 | **llama.cpp (GGUF + Metal)** | 115.63 | 115.6-115.6 | 0.00 | **15.6x** |
| 3위 | **PyTorch + MPS** | 7.42 | 7.4-7.4 | 0.00 | **1.0x** |

### 응답 속도 성능 (Latency)

| 순위 | 엔진 | 평균 Latency (초) | Latency 범위 | 표준편차 | 일관성 |
|------|------|-------------------|--------------|----------|--------|
| 1위 | **Ollama (GGUF)** | 3.862 | 3.86-3.86 | 0.000 | 높음 |
| 2위 | **llama.cpp (GGUF + Metal)** | 4.635 | 4.64-4.64 | 0.000 | 높음 |
| 3위 | **PyTorch + MPS** | 33.033 | 33.03-33.03 | 0.000 | 높음 |

## 📈 상세 성능 통계

### PyTorch + MPS

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 7.42 | 7.42 | 7.42 | 7.42 | 0.00 |
| **Latency (초)** | 33.033 | 33.033 | 33.033 | 33.033 | 0.000 |

### Ollama (GGUF)

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 132.07 | 132.07 | 132.07 | 132.07 | 0.00 |
| **Latency (초)** | 3.862 | 3.862 | 3.862 | 3.862 | 0.000 |

### llama.cpp (GGUF + Metal)

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 115.63 | 115.63 | 115.63 | 115.63 | 0.00 |
| **Latency (초)** | 4.635 | 4.635 | 4.635 | 4.635 | 0.000 |

## 🔍 성능 분석

### 성능 우수 엔진

- **최고 처리량**: Ollama (GGUF) (132.07 TPS)
- **최단 응답시간**: Ollama (GGUF) (3.862초)

### 성능 안정성 분석

1. **PyTorch + MPS**: TPS 변동성 0.0%, Latency 변동성 0.0% (안정성: 높음)
2. **Ollama (GGUF)**: TPS 변동성 0.0%, Latency 변동성 0.0% (안정성: 높음)
3. **llama.cpp (GGUF + Metal)**: TPS 변동성 0.0%, Latency 변동성 0.0% (안정성: 높음)

## 🎯 시나리오별 권장 엔진

### 처리량 우선 (배치 작업)
1. **Ollama (GGUF)**: 132.07 TPS, 3.9초 지연
2. **llama.cpp (GGUF + Metal)**: 115.63 TPS, 4.6초 지연
3. **PyTorch + MPS**: 7.42 TPS, 33.0초 지연

### 응답속도 우선 (실시간 대화)
1. **Ollama (GGUF)**: 3.862초 지연, 132.1 TPS
2. **llama.cpp (GGUF + Metal)**: 4.635초 지연, 115.6 TPS
3. **PyTorch + MPS**: 33.033초 지연, 7.4 TPS

### 안정성 우선 (프로덕션)
1. **PyTorch + MPS**: 전체 변동성 0.0%
2. **Ollama (GGUF)**: 전체 변동성 0.0%
3. **llama.cpp (GGUF + Metal)**: 전체 변동성 0.0%

## 📝 테스트 프롬프트

1. "파이썬에서 리스트와 튜플의 차이점은?"
## 📋 종합 결론

**종합 성능 순위** (TPS와 Latency 균등 가중치)

1. **Ollama (GGUF)**: 종합 점수 1.000 (처리량: 1.000, 응답속도: 1.000)
2. **llama.cpp (GGUF + Metal)**: 종합 점수 0.854 (처리량: 0.876, 응답속도: 0.833)
3. **PyTorch + MPS**: 종합 점수 0.087 (처리량: 0.056, 응답속도: 0.117)

---

**데이터 파일**
- JSON 데이터: `output/benchmark_results_1runs_quick_20250718_162001.json`
- 상세 로그: `logging/benchmark_detailed_20250718_162001.log`

*리포트 생성 시간: 2025-07-18T16:20:46.051243*
