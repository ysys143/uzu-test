# Uzu AI 추론 엔진 종합 성능 분석 리포트

## 📊 시스템 정보
- **모델**: MacBook Pro (Mac15,10)
- **프로세서**: Apple M3 Max
- **CPU 코어**: 14 (10 performance and 4 efficiency)
- **메모리**: 36 GB
- **운영체제**: macOS 15.5 (24F74)
- **Python**: 3.12.3

## ⚙️ 벤치마크 구성
- **실행 시간**: 2025-07-18T22:08:02.011596
- **반복 횟수**: 3회
- **테스트 프롬프트**: 10개
- **최대 토큰**: 500개
- **온도 설정**: 0.3 (일관된 응답)
- **총 실행 횟수**: 90회

## 🚀 성능 요약

### 처리량 성능 (TPS)

| 순위 | 엔진 | 평균 TPS | TPS 범위 | 표준편차 | 상대 성능 |
|------|------|----------|----------|----------|----------|
| 1위 | **Ollama (GGUF)** | 136.68 | 26.3-154.7 | 22.24 | **15.4x** |
| 2위 | **llama.cpp (GGUF + Metal)** | 121.02 | 20.2-139.6 | 20.52 | **13.6x** |
| 3위 | **PyTorch + MPS** | 8.87 | 7.6-9.2 | 0.39 | **1.0x** |

### 응답 속도 성능 (Latency)

| 순위 | 엔진 | 평균 Latency (초) | Latency 범위 | 표준편차 | 일관성 |
|------|------|-------------------|--------------|----------|--------|
| 1위 | **Ollama (GGUF)** | 4.858 | 2.69-18.01 | 2.976 | 낮음 |
| 2위 | **llama.cpp (GGUF + Metal)** | 5.025 | 2.94-21.95 | 3.427 | 낮음 |
| 3위 | **PyTorch + MPS** | 35.572 | 19.46-59.47 | 14.472 | 보통 |

## 📈 상세 성능 통계

### PyTorch + MPS

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 8.87 | 9.07 | 7.63 | 9.22 | 0.39 |
| **Latency (초)** | 35.572 | 28.404 | 19.456 | 59.475 | 14.472 |

### Ollama (GGUF)

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 136.68 | 141.13 | 26.26 | 154.74 | 22.24 |
| **Latency (초)** | 4.858 | 3.905 | 2.688 | 18.009 | 2.976 |

### llama.cpp (GGUF + Metal)

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 121.02 | 123.44 | 20.23 | 139.57 | 20.52 |
| **Latency (초)** | 5.025 | 4.007 | 2.943 | 21.946 | 3.427 |

## 🔍 성능 분석

### 성능 우수 엔진

- **최고 처리량**: Ollama (GGUF) (136.68 TPS)
- **최단 응답시간**: Ollama (GGUF) (4.858초)

### 성능 안정성 분석

1. **PyTorch + MPS**: TPS 변동성 4.4%, Latency 변동성 40.7% (안정성: 보통)
2. **Ollama (GGUF)**: TPS 변동성 16.3%, Latency 변동성 61.3% (안정성: 낮음)
3. **llama.cpp (GGUF + Metal)**: TPS 변동성 17.0%, Latency 변동성 68.2% (안정성: 낮음)

## 🎯 시나리오별 권장 엔진

### 처리량 우선 (배치 작업)
1. **Ollama (GGUF)**: 136.68 TPS, 4.9초 지연
2. **llama.cpp (GGUF + Metal)**: 121.02 TPS, 5.0초 지연
3. **PyTorch + MPS**: 8.87 TPS, 35.6초 지연

### 응답속도 우선 (실시간 대화)
1. **Ollama (GGUF)**: 4.858초 지연, 136.7 TPS
2. **llama.cpp (GGUF + Metal)**: 5.025초 지연, 121.0 TPS
3. **PyTorch + MPS**: 35.572초 지연, 8.9 TPS

### 안정성 우선 (프로덕션)
1. **PyTorch + MPS**: 전체 변동성 22.5%
2. **Ollama (GGUF)**: 전체 변동성 38.8%
3. **llama.cpp (GGUF + Metal)**: 전체 변동성 42.6%

## 📝 테스트 프롬프트

1. "파이썬에서 리스트와 튜플의 차이점은?"
2. "HTTP와 HTTPS의 차이점을 설명해주세요."
3. "Git과 GitHub의 차이점은?"
4. "머신러닝과 딥러닝의 차이는?"
5. "파이썬에서 효율적인 데이터 처리를 위한 라이브러리들과 각각의 특징을 설명해주세요."
6. "소프트웨어 개발에서 테스트 주도 개발(TDD)의 장점과 실제 적용 방법은?"
7. "DevOps 문화와 도구들이 소프트웨어 개발에 미치는 영향을 설명해주세요."
8. "현대 웹 개발의 전체 스택을 구성하는 기술들과 각 계층별 역할, 그리고 최신 트렌드를 종합적으로 설명해주세요."
9. "인공지능과 머신러닝 기술이 다양한 산업 분야에 미치는 영향과 구체적인 활용 사례들, 그리고 향후 발전 방향과 고려해야 할 윤리적 이슈들을 설명해주세요."
10. "게임 개발에서 사용되는 엔진들의 특징과 그래픽스 파이프라인, 물리 엔진, AI 시스템 구현 방법, 그리고 최적화 기법들을 개발 단계별로 설명해주세요."

## 📊 실행별 성능 추이

### PyTorch + MPS

| 실행 | 평균 TPS | 평균 Latency (초) |
|------|----------|-------------------|
| 1 | 8.58 | 36.934 |
| 2 | 8.98 | 34.942 |
| 3 | 9.06 | 34.839 |

### Ollama (GGUF)

| 실행 | 평균 TPS | 평균 Latency (초) |
|------|----------|-------------------|
| 1 | 130.17 | 5.783 |
| 2 | 139.32 | 4.280 |
| 3 | 140.54 | 4.511 |

### llama.cpp (GGUF + Metal)

| 실행 | 평균 TPS | 평균 Latency (초) |
|------|----------|-------------------|
| 1 | 117.43 | 6.338 |
| 2 | 124.20 | 4.459 |
| 3 | 121.42 | 4.280 |

## 📋 종합 결론

**종합 성능 순위** (TPS와 Latency 균등 가중치)

1. **Ollama (GGUF)**: 종합 점수 1.000 (처리량: 1.000, 응답속도: 1.000)
2. **llama.cpp (GGUF + Metal)**: 종합 점수 0.926 (처리량: 0.885, 응답속도: 0.967)
3. **PyTorch + MPS**: 종합 점수 0.101 (처리량: 0.065, 응답속도: 0.137)

---

**데이터 파일**
- JSON 데이터: `output/subprocess_benchmark_results_3runs_20250718_214515.json`
- 상세 로그: `logging/subprocess_benchmark_detailed_20250718_214515.log`

*리포트 생성 시간: 2025-07-18T22:08:02.011596*
