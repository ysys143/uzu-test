# Uzu AI 추론 엔진 종합 성능 분석 리포트

## 📊 시스템 정보
- **모델**: MacBook Pro (Mac15,10)
- **프로세서**: Apple M3 Max
- **CPU 코어**: 14 (10 performance and 4 efficiency)
- **메모리**: 36 GB
- **운영체제**: macOS 15.5 (24F74)
- **Python**: 3.12.3

## ⚙️ 벤치마크 구성
- **실행 시간**: 2025-07-18T17:09:02.614964
- **반복 횟수**: 1회
- **테스트 프롬프트**: 1개
- **최대 토큰**: 500개
- **온도 설정**: 0.3 (일관된 응답)
- **총 실행 횟수**: 3회

## 🚀 성능 요약

### 처리량 성능 (TPS)

| 순위 | 엔진 | 평균 TPS | TPS 범위 | 표준편차 | 상대 성능 |
|------|------|----------|----------|----------|----------|
| 1위 | **llama.cpp (GGUF + Metal)** | 120.04 | 120.0-120.0 | 0.00 | **16.6x** |
| 2위 | **Ollama (GGUF)** | 102.04 | 102.0-102.0 | 0.00 | **14.1x** |
| 3위 | **PyTorch + MPS** | 7.22 | 7.2-7.2 | 0.00 | **1.0x** |

### 응답 속도 성능 (Latency)

| 순위 | 엔진 | 평균 Latency (초) | Latency 범위 | 표준편차 | 일관성 |
|------|------|-------------------|--------------|----------|--------|
| 1위 | **llama.cpp (GGUF + Metal)** | 4.249 | 4.25-4.25 | 0.000 | 높음 |
| 2위 | **Ollama (GGUF)** | 4.998 | 5.00-5.00 | 0.000 | 높음 |
| 3위 | **PyTorch + MPS** | 33.245 | 33.24-33.24 | 0.000 | 높음 |

## 📈 상세 성능 통계

### PyTorch + MPS

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 7.22 | 7.22 | 7.22 | 7.22 | 0.00 |
| **Latency (초)** | 33.245 | 33.245 | 33.245 | 33.245 | 0.000 |

### Ollama (GGUF)

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 102.04 | 102.04 | 102.04 | 102.04 | 0.00 |
| **Latency (초)** | 4.998 | 4.998 | 4.998 | 4.998 | 0.000 |

### llama.cpp (GGUF + Metal)

| 지표 | 평균 | 중간값 | 최소값 | 최대값 | 표준편차 |
|------|------|--------|--------|--------|---------|
| **TPS** | 120.04 | 120.04 | 120.04 | 120.04 | 0.00 |
| **Latency (초)** | 4.249 | 4.249 | 4.249 | 4.249 | 0.000 |

## 🔍 성능 분석

### 성능 우수 엔진

- **최고 처리량**: llama.cpp (GGUF + Metal) (120.04 TPS)
- **최단 응답시간**: llama.cpp (GGUF + Metal) (4.249초)

### 성능 안정성 분석

1. **PyTorch + MPS**: TPS 변동성 0.0%, Latency 변동성 0.0% (안정성: 높음)
2. **Ollama (GGUF)**: TPS 변동성 0.0%, Latency 변동성 0.0% (안정성: 높음)
3. **llama.cpp (GGUF + Metal)**: TPS 변동성 0.0%, Latency 변동성 0.0% (안정성: 높음)

## 🎯 시나리오별 권장 엔진

### 처리량 우선 (배치 작업)
1. **llama.cpp (GGUF + Metal)**: 120.04 TPS, 4.2초 지연
2. **Ollama (GGUF)**: 102.04 TPS, 5.0초 지연
3. **PyTorch + MPS**: 7.22 TPS, 33.2초 지연

### 응답속도 우선 (실시간 대화)
1. **llama.cpp (GGUF + Metal)**: 4.249초 지연, 120.0 TPS
2. **Ollama (GGUF)**: 4.998초 지연, 102.0 TPS
3. **PyTorch + MPS**: 33.245초 지연, 7.2 TPS

### 안정성 우선 (프로덕션)
1. **PyTorch + MPS**: 전체 변동성 0.0%
2. **Ollama (GGUF)**: 전체 변동성 0.0%
3. **llama.cpp (GGUF + Metal)**: 전체 변동성 0.0%

## 📝 테스트 프롬프트

1. "파이썬에서 리스트와 튜플의 차이점은?"
## 📋 종합 결론

**종합 성능 순위** (TPS와 Latency 균등 가중치)

1. **llama.cpp (GGUF + Metal)**: 종합 점수 1.000 (처리량: 1.000, 응답속도: 1.000)
2. **Ollama (GGUF)**: 종합 점수 0.850 (처리량: 0.850, 응답속도: 0.850)
3. **PyTorch + MPS**: 종합 점수 0.094 (처리량: 0.060, 응답속도: 0.128)

---

**데이터 파일**
- JSON 데이터: `output/subprocess_benchmark_results_1runs_quick_20250718_170816.json`
- 상세 로그: `logging/subprocess_benchmark_detailed_20250718_170816.log`

*리포트 생성 시간: 2025-07-18T17:09:02.614964*
