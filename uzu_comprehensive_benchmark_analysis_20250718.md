# Uzu AI 추론 엔진 종합 성능 분석 보고서

## 📊 개요

이 보고서는 Apple Silicon M3 Max 환경에서 다양한 AI 추론 엔진들의 성능을 **CLI 모드(Subprocess)**와 **API 모드(Server)** 두 가지 방식으로 측정한 종합 분석 결과입니다.

### 🎯 테스트 환경
- **하드웨어**: MacBook Pro (Mac15,10), Apple M3 Max, 14코어 CPU, 36GB RAM
- **운영체제**: macOS 15.5 (24F74)
- **테스트 모델**: Google Gemma-3-1B-IT
- **실행 횟수**: 각 모드별 3회 반복, 10개 프롬프트 테스트
- **측정 지표**: TPS (Tokens Per Second), 추론 시간(Latency)

### 🔧 테스트 엔진
1. **PyTorch + MPS**: HuggingFace Transformers + Apple Metal Performance Shaders
2. **Ollama**: GGUF 모델 기반 추론 엔진
3. **llama.cpp**: Metal 가속 CLI/서버 추론 엔진  
4. **Uzu**: Rust 기반 네이티브 Metal 추론 엔진 (API 모드만)

---

## 🚀 핵심 성능 결과

### CLI 모드 (Subprocess) 성능
| 순위 | 엔진 | 평균 TPS | 상대 성능 | 평균 Latency | 안정성 |
|------|------|----------|----------|--------------|--------|
| 🥇 | **Ollama** | 136.68 | 15.4x | 4.86초 | 보통 |
| 🥈 | **llama.cpp** | 121.02 | 13.6x | 5.03초 | 보통 |
| 🥉 | **PyTorch** | 8.87 | 1.0x | 35.57초 | 좋음 |

### API 모드 (Server) 성능
| 순위 | 엔진 | 평균 TPS | 상대 성능 | 평균 Latency | 안정성 |
|------|------|----------|----------|--------------|--------|
| 🥇 | **llama.cpp** | 75.69 | 9.2x | 5.76초 | 좋음 |
| 🥈 | **Ollama** | 72.97 | 8.9x | 4.57초 | 보통 |
| 🥉 | **Uzu** | 35.36 | 4.3x | 5.48초 | 좋음 |
| 4위 | **PyTorch** | 8.20 | 1.0x | 50.74초 | 낮음 |

---

## 📈 상세 성능 분석

### 1. 처리량 성능 (TPS) 비교

#### CLI vs API 모드 TPS 비교
| 엔진 | CLI 모드 TPS | API 모드 TPS | 성능 차이 | 분석 |
|------|-------------|-------------|----------|------|
| **Ollama** | 136.68 | 72.97 | -46.6% | CLI 모드에서 HTTP 오버헤드 없이 최적 성능 |
| **llama.cpp** | 121.02 | 75.69 | -37.5% | 서버 모드에서도 안정적인 고성능 유지 |
| **PyTorch** | 8.87 | 8.20 | -7.6% | 두 모드 간 성능 차이 미미 |

#### 성능 순위 변화
- **CLI 모드**: Ollama > llama.cpp > PyTorch
- **API 모드**: llama.cpp > Ollama > Uzu > PyTorch

### 2. 응답 속도 (Latency) 비교

#### CLI vs API 모드 Latency 비교  
| 엔진 | CLI 모드 | API 모드 | 차이 | 분석 |
|------|----------|----------|------|------|
| **Ollama** | 4.86초 | 4.57초 | +6.0% | 서버 모드에서 약간 더 빠른 응답 |
| **llama.cpp** | 5.03초 | 5.76초 | -14.5% | HTTP 오버헤드로 약간 지연 |
| **PyTorch** | 35.57초 | 50.74초 | -42.7% | 서버 모드에서 성능 저하 심각 |

### 3. 성능 안정성 분석

#### 변동성 계수 (CV) 비교
| 엔진 | CLI 모드 안정성 | API 모드 안정성 | 우수 모드 |
|------|----------------|----------------|----------|
| **PyTorch** | 보통 (22.5%) | 낮음 (106.0%) | CLI 모드 |
| **Ollama** | 낮음 (38.8%) | 낮음 (78.2%) | CLI 모드 |
| **llama.cpp** | 낮음 (42.6%) | 좋음 (21.7%) | **API 모드** |
| **Uzu** | - | 좋음 (19.7%) | API 전용 |

---

## 🔍 성능 특성 분석

### 처리량 중심 분석
- **최고 TPS**: Ollama CLI (136.68 TPS)
- **CLI 모드 우위**: Ollama, llama.cpp 모두 CLI에서 더 높은 TPS
- **API 오버헤드**: 평균 20-40% 성능 저하

### 응답 속도 중심 분석  
- **최단 Latency**: Ollama API (4.57초)
- **CLI vs API**: Ollama는 API가 더 빠름, llama.cpp은 CLI가 더 빠름
- **PyTorch 특이점**: API 모드에서 42.7% 응답 시간 증가

### 안정성 중심 분석
- **가장 안정적**: llama.cpp API (변동성 21.7%)
- **PyTorch 특성**: CLI 모드에서 안정성 우수
- **변동성 패턴**: 대부분 엔진에서 API 모드 변동성 증가

---

## 🔍 주요 인사이트

### 1. CLI vs API 성능 차이 원인
- **HTTP 오버헤드**: API 모드에서 평균 20-40% 성능 저하
- **프로세스 격리**: 서버 모드에서 메모리 및 CPU 자원 분산
- **동시성 처리**: API 서버는 멀티 클라이언트 대응으로 단일 요청 성능 트레이드오프

### 2. 엔진별 특성 분석

#### Ollama
- **장점**: CLI 모드에서 최고 성능, 사용 편의성
- **단점**: 성능 변동성이 상대적으로 높음

#### llama.cpp  
- **장점**: CLI/API 모두 균형잡힌 고성능, API 모드 안정성 우수
- **단점**: 설정 복잡도 상대적으로 높음

#### PyTorch
- **장점**: 성능 안정성, 개발 생태계 우수
- **단점**: 절대 성능이 다른 엔진 대비 현저히 낮음

#### Uzu
- **장점**: 네이티브 Metal 최적화, 중간 수준 성능
- **단점**: CLI 모드 미지원, 상대적으로 신기술

### 3. Apple Silicon 최적화 효과
- **Metal 가속**: llama.cpp, Ollama에서 10-15배 성능 향상 확인
- **MPS 백엔드**: PyTorch도 Metal 가속 적용되지만 상대적으로 제한적
- **네이티브 최적화**: Uzu의 Rust+Metal 조합으로 중간 성능 달성

---

## 📊 종합 성능 순위

### CLI 모드 종합 점수
1. **Ollama**: 1.000점 (처리량 1.000, 응답속도 1.000)
2. **llama.cpp**: 0.926점 (처리량 0.885, 응답속도 0.967)
3. **PyTorch**: 0.101점 (처리량 0.065, 응답속도 0.137)

### API 모드 종합 점수  
1. **Ollama**: 0.982점 (처리량 0.964, 응답속도 1.000)
2. **llama.cpp**: 0.897점 (처리량 1.000, 응답속도 0.794)
3. **Uzu**: 0.651점 (처리량 0.467, 응답속도 0.835)
4. **PyTorch**: 0.099점 (처리량 0.108, 응답속도 0.090)

---

## 📋 측정 결과 요약

### 🏆 성능 순위

**CLI 모드 (절대 성능)**
1. **Ollama**: 136.68 TPS, 4.86초 응답시간
2. **llama.cpp**: 121.02 TPS, 5.03초 응답시간  
3. **PyTorch**: 8.87 TPS, 35.57초 응답시간

**API 모드 (서버 성능)**
1. **llama.cpp**: 75.69 TPS, 5.76초 응답시간
2. **Ollama**: 72.97 TPS, 4.57초 응답시간
3. **Uzu**: 35.36 TPS, 5.48초 응답시간
4. **PyTorch**: 8.20 TPS, 50.74초 응답시간

### 📊 주요 발견사항

1. **Metal 가속 효과**: llama.cpp, Ollama에서 PyTorch 대비 10-15배 성능 향상
2. **API 오버헤드**: HTTP 서버 모드에서 평균 20-40% 성능 저하
3. **안정성 차이**: llama.cpp API 모드가 가장 안정적 (변동성 21.7%)
4. **Uzu 특징**: API 전용, 중간 성능, 네이티브 Metal 최적화

---

**보고서 생성 시간**: 2025-07-18T22:30:00  
**데이터 기반**: API 벤치마크(2025-07-18T21:45:11), Subprocess 벤치마크(2025-07-18T22:08:02)  
**총 테스트 실행**: 210회 (API 120회 + Subprocess 90회) 