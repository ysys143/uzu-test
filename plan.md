# Uzu AI 추론 엔진 성능 벤치마크 계획

## 프로젝트 개요
- **목표**: Uzu, Ollama, llama.cpp, PyTorch의 성능을 비교하여 Apple Silicon에서의 AI 추론 성능 분석
- **기간**: 2025년 7월 17일 ~ 8월 15일 (약 4주)
- **환경**: macOS (Apple Silicon)

## 1단계: 환경 설정 및 프로젝트 준비 (7/17 - 7/20)

### 1.1 Uzu 프로젝트 설정
- [x] Uzu 프로젝트 클론 완료
- [x] Rust 툴체인 설정 확인 (rust-toolchain.toml 기반) - Rust 1.86.0 설치됨
- [x] 의존성 설치 및 빌드 테스트 - Xcode 라이센스 동의 후 빌드 성공 ✅
- [x] CLI 도구 기능 확인 - uzu_cli 정상 작동 확인 ✅

### 1.2 비교 대상 도구 설치
- [x] Ollama 설치 및 설정 - v0.9.3 ✅
- [x] llama.cpp 빌드 및 설정 (Metal 지원 포함) - v5910, Metal 가속 확인 ✅
- [x] PyTorch 환경 구성 (Apple Silicon 최적화) - v2.7.1, MPS 백엔드 활성화 ✅
- [x] lalamo 설치 및 설정 - 모델 변환 도구 준비 완료 ✅

### 1.3 개발 환경 구성
- [x] 성능 측정 스크립트 개발 환경 준비 - Python 가상환경 구성 완료 ✅
- [ ] 벤치마크 결과 저장용 디렉토리 구조 생성

## 2단계: 모델 준비 및 변환 (7/21 - 7/25)

### 2.1 대상 모델 선정
**주요 테스트 모델:**
- [ ] **Gemma-3-1B-IT** (통일된 원본 모델)
  - 더 작은 크기로 빠른 테스트 가능
  - 모든 엔진에서 동일한 원본 모델 사용
  - 공정한 성능 비교 보장

### 2.2 모델 다운로드 및 변환
- [ ] **HuggingFace 원본 모델 다운로드**
  - `google/gemma-3-1b-it` (원본 float16 형식)
  - 모든 엔진이 동일한 소스 사용
- [ ] **각 엔진별 형식 준비**
  - **PyTorch**: HuggingFace 원본 모델 직접 사용 (변환 불필요) ✅
  - **Uzu**: HuggingFace safetensors 형식 직접 사용 (변환 불필요) ✅
  - **Ollama**: HF → GGUF 변환 후 Modelfile로 등록 ⚠️
  - **llama.cpp**: HF → GGUF 변환 필요 ⚠️
    ```bash
    # 공통 GGUF 변환 (Ollama와 llama.cpp 모두 사용)
    python convert_hf_to_gguf.py models/gemma-3-1b-it/ --outfile models/gemma-3-1b-it.gguf --outtype f16
    ```

### 2.3 모델 검증
- [ ] **각 엔진에서 동일 모델 로딩 테스트**
  - Uzu: safetensors 형식 정상 로딩
  - llama.cpp: GGUF 형식 정상 로딩  
  - Ollama: HF 모델 정상 작동
  - PyTorch: MPS 백엔드 정상 작동
- [ ] **일관된 추론 결과 확인**
  - 동일 프롬프트로 각 엔진 테스트
  - 출력 품질 일관성 검증

## 3단계: 벤치마크 도구 개발 (7/26 - 8/1)

### 3.1 공통 테스트 시나리오 정의
- [ ] 표준 프롬프트 세트 정의
  - 짧은 응답 (50 토큰)
  - 중간 응답 (200 토큰)
  - 긴 응답 (500 토큰)
- [ ] 성능 메트릭 정의
  - Tokens per second (TPS)
  - Time to First Token (TTFT)
  - 메모리 사용량
  - CPU/GPU 사용률

### 3.2 벤치마크 스크립트 개발
- [ ] Uzu 성능 측정 스크립트 (Rust)
- [ ] Ollama API 기반 성능 측정 스크립트
- [ ] llama.cpp 성능 측정 스크립트
- [ ] PyTorch 성능 측정 스크립트
- [ ] 결과 수집 및 통계 처리 도구

### 3.3 자동화 도구 구성
- [ ] 전체 벤치마크 자동 실행 스크립트
- [ ] 결과 시각화 도구 (차트, 그래프)
- [ ] 시스템 리소스 모니터링 도구

## 4단계: 성능 측정 실행 (8/2 - 8/8)

### 4.1 기준 성능 측정
- [ ] 시스템 기준 성능 측정
  - 메모리 대역폭
  - Metal 성능 특성
  - CPU 벤치마크

### 4.2 엔진별 성능 측정
**각 엔진별로 실행:**
- [ ] Uzu 성능 측정
  - 다양한 정밀도 (f16, bf16)
  - GPU/ANE 활용도 분석
- [ ] Ollama 성능 측정
- [ ] llama.cpp 성능 측정
  - Metal 가속 vs CPU
- [ ] PyTorch 성능 측정
  - MPS 백엔드 활용

### 4.3 상세 분석
- [ ] 병목 지점 분석
- [ ] 메모리 사용 패턴 분석
- [ ] 배치 크기별 성능 차이
- [ ] 모델 크기별 스케일링 특성

## 5단계: 결과 분석 및 보고서 작성 (8/9 - 8/15)

### 5.1 데이터 분석
- [ ] 성능 메트릭별 비교 분석
- [ ] 통계적 유의성 검증
- [ ] 사용 시나리오별 최적 엔진 추천

### 5.2 보고서 작성
- [ ] 벤치마크 결과 정리
- [ ] 성능 차이의 원인 분석
- [ ] 각 엔진의 장단점 요약
- [ ] 실제 사용 시나리오별 권장사항

### 5.3 최종 정리
- [ ] 코드 정리 및 문서화
- [ ] 재현 가능한 벤치마크 스크립트 패키징
- [ ] 결과물 아카이브

## 기술적 고려사항

### 성능 측정 정확성
- 워밍업 과정 포함
- 여러 회 반복 측정 후 평균값 사용
- 시스템 부하 최소화 상태에서 측정
- 일관된 측정 조건 유지

### 공정한 비교를 위한 조건
- **동일한 원본 모델 사용**: google/gemma-3-1b-it
- **동일한 정밀도**: float16 (각 엔진의 기본 최적화 활용)
- **동일한 추론 설정**: temperature=0.8, top_p=0.95
- **동일한 하드웨어 환경**: Apple Silicon Mac
- **동일한 시스템 리소스 상태**: 측정 시 일관된 조건 유지

## 예상 결과 및 가설

### 가설
1. **Uzu**: Apple Silicon 특화 최적화로 인한 높은 성능 예상
2. **llama.cpp**: 범용성과 안정성, Metal 지원으로 준수한 성능
3. **Ollama**: 사용 편의성과 합리적 성능의 균형
4. **PyTorch**: 유연성은 높지만 추론 특화 최적화 부족으로 상대적으로 낮은 성능

### 기대 성과
- Apple Silicon에서의 AI 추론 엔진별 명확한 성능 비교 데이터
- 사용 시나리오별 최적 도구 선택 가이드라인
- Uzu의 실제 성능 우위 검증