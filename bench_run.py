#!/usr/bin/env python3
"""
Uzu AI ì¶”ë¡  ì—”ì§„ ë‹¤ì¤‘ ì‹¤í–‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ê° ì—”ì§„ë³„ë¡œ 10ë²ˆ ë°˜ë³µ ì‹¤í–‰í•˜ì—¬ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì§€ì› ì—”ì§„:
- PyTorch (HuggingFace + MPS)
- Ollama (GGUF)
- llama.cpp (GGUF + Metal)
- Uzu (Native Metal)
"""

# HuggingFace tokenizers ë³‘ë ¬ì²˜ë¦¬ ê²½ê³  ì–µì œ
import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import time
import subprocess
import json
import statistics
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import requests
import logging


class MultiRunBenchmarkRunner:
    def __init__(self, num_runs: int = None, config_file: str = "benchmark_config.json", quick_test: bool = False):
        # ì„¤ì • íŒŒì¼ ë¡œë”©
        self.config = self._load_config(config_file)
        
        # í™˜ê²½ë³€ìˆ˜ì™€ ì„¤ì • íŒŒì¼ì—ì„œ ë§¤ê°œë³€ìˆ˜ ë¡œë”©
        self.num_runs = num_runs or int(os.getenv('BENCHMARK_NUM_RUNS', self.config['benchmark']['num_runs']))
        self.max_tokens = int(os.getenv('BENCHMARK_MAX_TOKENS', self.config['benchmark']['max_tokens']))
        self.temperature = float(os.getenv('BENCHMARK_TEMPERATURE', self.config['benchmark']['temperature']))
        self.timeout_seconds = int(os.getenv('BENCHMARK_TIMEOUT', self.config['benchmark']['timeout_seconds']))
        self.quick_test = quick_test or os.getenv('BENCHMARK_QUICK_TEST', '').lower() in ('true', '1', 'yes')
        
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        logging_config = self.config['logging']
        os.makedirs(logging_config['directory'], exist_ok=True)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('report', exist_ok=True)
        os.makedirs('output', exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_filename = f"{logging_config['directory']}/benchmark_detailed_{timestamp}.log"
        self.timestamp = timestamp
        
        # ë¡œê¹… ì„¤ì •
        handlers = [logging.FileHandler(self.log_filename, encoding='utf-8')]
        if logging_config.get('console_output', True):
            handlers.append(logging.StreamHandler())
            
        logging.basicConfig(
            level=getattr(logging, logging_config.get('level', 'INFO')),
            format='%(asctime)s - %(message)s',
            handlers=handlers
        )
        self.logger = logging.getLogger(__name__)
        
        # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ë„ì…
        from benchmark_prompts import get_all_prompts, SYSTEM_PROMPT
        all_prompts = get_all_prompts()
        
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
        if self.quick_test:
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©
            self.test_prompts = [all_prompts[0]]  # íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œ ì°¨ì´
        else:
            # ì •ì‹ í…ŒìŠ¤íŠ¸: ë‹¤ì–‘í•œ ê¸¸ì´ì˜ í”„ë¡¬í”„íŠ¸ 10ê°œ ì„ íƒ (ë” í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸)
            self.test_prompts = [
                # ì§§ì€ í”„ë¡¬í”„íŠ¸ (4ê°œ)
                all_prompts[0],   # íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œ ì°¨ì´
                all_prompts[1],   # HTTPì™€ HTTPS ì°¨ì´  
                all_prompts[4],   # Gitê³¼ GitHub ì°¨ì´
                all_prompts[8],   # ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ì°¨ì´
                
                # ì¤‘ê°„ í”„ë¡¬í”„íŠ¸ (3ê°œ)
                all_prompts[18],  # ë°ì´í„° ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
                all_prompts[22],  # ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜ íŒ¨í„´
                all_prompts[25],  # í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ë¹„êµ
                
                # ê¸´ í”„ë¡¬í”„íŠ¸ (3ê°œ)  
                all_prompts[34],  # ì›¹ ê°œë°œ ìŠ¤íƒ
                all_prompts[40],  # ë¶„ì‚° ì‹œìŠ¤í…œ ì„¤ê³„
                all_prompts[45],  # ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸
            ]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
        system_prompt_override = os.getenv('BENCHMARK_SYSTEM_PROMPT') or self.config['benchmark'].get('system_prompt_override')
        self.system_prompt = system_prompt_override if system_prompt_override else SYSTEM_PROMPT
        
        self.results = {}
        
    def _load_config(self, config_file: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë”©"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë”© ì„±ê³µ: {config_file}")
            return config
        except FileNotFoundError:
            print(f"âš ï¸  ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self._get_default_config()
        except json.JSONDecodeError as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            "benchmark": {
                "max_tokens": 500,
                "temperature": 0.3,
                "num_runs": 10,
                "timeout_seconds": 120,
                "system_prompt_override": None
            },
            "engines": {
                "pytorch": {"enabled": True, "device": "mps", "torch_dtype": "float16"},
                "ollama": {"enabled": True, "model_name": "gemma-3-1b-it-bench", "verbose": True},
                "llamacpp": {"enabled": True, "model_path": "./models/gemma-3-1b-it-gguf-llama/model.gguf", "ngl": 99, "chat_template": "gemma"},
                "uzu": {"enabled": True, "model_path": "./models/gemma-3-1b-it-uzu", "port": 51839, "server_timeout": 60}
            },
            "logging": {"directory": "logging", "level": "INFO", "console_output": True},
            "output": {"report_directory": "report", "data_directory": "output"}
        }
        
    def log_response_details(self, engine_name: str, prompt_idx: int, inference_time: float, 
                           tps: float, response_text: str, tokens_count: int = None):
        """ì‘ë‹µ ìƒì„¸ ì •ë³´ë¥¼ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡"""
        response_length = len(response_text.split()) if response_text else 0
        
        # í™”ë©´ì—ëŠ” ê°„ë‹¨í•œ ì •ë³´ë§Œ
        print(f"    í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}: {inference_time:.3f}ì´ˆ, {tps:.2f} TPS")
        
        # ë¡œê·¸ íŒŒì¼ì—ëŠ” ìƒì„¸ ì •ë³´
        self.logger.info(f"[{engine_name}] í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}")
        self.logger.info(f"[{engine_name}] ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ")
        if tokens_count:
            self.logger.info(f"[{engine_name}] ìƒì„± í† í°: {tokens_count}ê°œ")
        self.logger.info(f"[{engine_name}] TPS: {tps:.2f}")
        self.logger.info(f"[{engine_name}] ì‘ë‹µ ê¸¸ì´: {response_length}ë‹¨ì–´, {len(response_text)}ì")
        self.logger.info(f"[{engine_name}] ì‘ë‹µ ë‚´ìš©: {response_text}")
        self.logger.info(f"[{engine_name}] " + "-" * 60)
        
    def calculate_statistics(self, values: List[float]) -> Dict:
        """í†µê³„ ê³„ì‚°"""
        if not values:
            return {
                'mean': 0,
                'median': 0,
                'min': 0,
                'max': 0,
                'std': 0,
                'count': 0
            }
            
        return {
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'min': min(values),
            'max': max(values),
            'std': statistics.stdev(values) if len(values) > 1 else 0,
            'count': len(values)
        }
        
    def test_pytorch_mps_multi_run(self):
        """PyTorch + MPS ë‹¤ì¤‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"ğŸ”¥ PyTorch + MPS {self.num_runs}íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ëª¨ë¸ í•œ ë²ˆë§Œ ë¡œë”©
        print("  ëª¨ë¸ ë¡œë”© ì¤‘...")
        load_start = time.time()
        tokenizer = AutoTokenizer.from_pretrained('./models/gemma-3-1b-it')
        model = AutoModelForCausalLM.from_pretrained(
            './models/gemma-3-1b-it',
            torch_dtype=torch.float16,
            device_map="mps"
        )
        load_time = time.time() - load_start
        print(f"  ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
        
        all_runs = []
        tps_values = []
        inference_times = []
        
        for run_idx in range(self.num_runs):
            print(f"  ì‹¤í–‰ {run_idx + 1}/{self.num_runs}...")
            run_results = []
            
            for prompt_idx, prompt in enumerate(self.test_prompts):
                print(f"    í”„ë¡¬í”„íŠ¸ {prompt_idx + 1} ì‹œì‘...")
                try:
                    # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                    inference_start = time.time()
                    
                    # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì‚¬ìš©ì ë©”ì‹œì§€)
                    messages = [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": prompt}
                    ]
                    
                    formatted_prompt = tokenizer.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    inputs = tokenizer(formatted_prompt, return_tensors="pt").to("mps")
                    with torch.no_grad():
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=self.max_tokens,
                            do_sample=True,
                            temperature=self.temperature,
                            pad_token_id=tokenizer.eos_token_id
                        )
                    
                    # ìƒì„±ëœ í† í°ë§Œ ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
                    input_length = inputs['input_ids'].shape[1]
                    generated_tokens = outputs[0][input_length:]
                    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    inference_time = time.time() - inference_start
                    
                    # ìƒì„±ëœ í† í° ìˆ˜ ê³„ì‚°
                    output_tokens = len(generated_tokens)
                    tps = output_tokens / inference_time if inference_time > 0 else 0
                    
                    # ì‘ë‹µ ìƒì„¸ ë¡œê¹…
                    self.log_response_details("PyTorch", prompt_idx, inference_time, tps, response, output_tokens)
                    
                    run_results.append({
                        'prompt_idx': prompt_idx,
                        'prompt': prompt,
                        'response': response,
                        'inference_time': inference_time,
                        'tokens_generated': output_tokens,
                        'tps': tps
                    })
                    
                    tps_values.append(tps)
                    inference_times.append(inference_time)
                    
                    # ë³€ìˆ˜ ì •ë¦¬ (ë©”ëª¨ë¦¬ ìºì‹œëŠ” ë‚˜ì¤‘ì— ì •ë¦¬)
                    del outputs, inputs, generated_tokens
                    
                except Exception as e:
                    print(f"    í”„ë¡¬í”„íŠ¸ {prompt_idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
                
                print(f"    í”„ë¡¬í”„íŠ¸ {prompt_idx + 1} ì™„ë£Œ, ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë¡œ...")
            
            # ì´ë²ˆ ì‹¤í–‰ì˜ í‰ê·  TPS
            if run_results:
                run_avg_tps = sum(r['tps'] for r in run_results) / len(run_results)
                all_runs.append({
                    'run_index': run_idx,
                    'avg_tps': run_avg_tps,
                    'tests': run_results
                })
                print(f"    ì‹¤í–‰ {run_idx + 1} í‰ê·  TPS: {run_avg_tps:.2f}")
            else:
                print(f"    ì‹¤í–‰ {run_idx + 1}: ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì‹¤íŒ¨")
        
        # í†µê³„ ê³„ì‚°
        run_avg_tps_values = [run['avg_tps'] for run in all_runs]
        
        self.results['pytorch'] = {
            'engine': 'PyTorch + MPS',
            'num_runs': self.num_runs,
            'load_time': load_time,
            'statistics': {
                'tps': self.calculate_statistics(tps_values),
                'inference_time': self.calculate_statistics(inference_times),
                'run_avg_tps': self.calculate_statistics(run_avg_tps_values)
            },
            'all_runs': all_runs
        }
        
        avg_tps = self.results['pytorch']['statistics']['tps']['mean']
        print(f"âœ… PyTorch ì™„ë£Œ - ë¡œë”©: {load_time:.2f}ì´ˆ, ì „ì²´ í‰ê·  TPS: {avg_tps:.2f}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, tokenizer
        torch.mps.empty_cache()
        
    def test_ollama_multi_run(self):
        """Ollama ë‹¤ì¤‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ¦™ Ollama {self.num_runs}íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        all_runs = []
        tps_values = []
        inference_times = []
        
        for run_idx in range(self.num_runs):
            print(f"  ì‹¤í–‰ {run_idx + 1}/{self.num_runs}...")
            run_results = []
            
            for prompt_idx, prompt in enumerate(self.test_prompts):
                # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                full_prompt = f"{self.system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
                
                ollama_config = self.config['engines']['ollama']
                cmd = ['ollama', 'run', ollama_config['model_name']]
                if ollama_config.get('verbose', False):
                    cmd.append('--verbose')
                
                start_time = time.time()
                try:
                    result = subprocess.run(
                        cmd, 
                        capture_output=True, 
                        text=True, 
                        timeout=self.timeout_seconds,
                        input=full_prompt + '\n'
                    )
                    inference_time = time.time() - start_time
                    
                    if result.returncode == 0:
                        output_lines = result.stdout.strip().split('\n')
                        stderr_lines = result.stderr.strip().split('\n')
                        response = ""
                        eval_rate = 0
                        
                        # stderrì—ì„œ ì„±ëŠ¥ ì •ë³´ íŒŒì‹±
                        for line in stderr_lines:
                            if 'eval rate:' in line:
                                try:
                                    # "eval rate:     77.72 tokens/s" í˜•ì‹ íŒŒì‹±
                                    parts = line.split('eval rate:')[1].strip()
                                    eval_rate = float(parts.split('tokens/s')[0].strip())
                                except Exception as e:
                                    print(f"      ì„±ëŠ¥ íŒŒì‹± ì˜¤ë¥˜: {line} -> {e}")
                                    pass
                        
                        # stdoutì—ì„œ ì‘ë‹µ í…ìŠ¤íŠ¸ íŒŒì‹±
                        for line in output_lines:
                            if not any(keyword in line for keyword in ['total duration:', 'load duration:', 'prompt eval', 'eval ', 'rate:']):
                                if line.strip() and not line.startswith('>>>'):
                                    response += line + " "
                        
                        # ì‘ë‹µ ìƒì„¸ ë¡œê¹…
                        self.log_response_details("Ollama", prompt_idx, inference_time, eval_rate, response)
                        
                        run_results.append({
                            'prompt_idx': prompt_idx,
                            'prompt': prompt,
                            'response': response.strip(),
                            'inference_time': inference_time,
                            'tps': eval_rate
                        })
                        
                        tps_values.append(eval_rate)
                        inference_times.append(inference_time)
                    else:
                        print(f"    ì—ëŸ¬ (í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}): {result.stderr}")
                        
                except subprocess.TimeoutExpired:
                    print(f"    íƒ€ì„ì•„ì›ƒ (í”„ë¡¬í”„íŠ¸ {prompt_idx + 1})")
            
            if run_results:
                run_avg_tps = sum(r['tps'] for r in run_results) / len(run_results)
                all_runs.append({
                    'run_index': run_idx,
                    'avg_tps': run_avg_tps,
                    'tests': run_results
                })
                print(f"    ì‹¤í–‰ {run_idx + 1} í‰ê·  TPS: {run_avg_tps:.2f}")
        
        # í†µê³„ ê³„ì‚°
        run_avg_tps_values = [run['avg_tps'] for run in all_runs]
        
        self.results['ollama'] = {
            'engine': 'Ollama (GGUF)',
            'num_runs': self.num_runs,
            'load_time': 0,  # OllamaëŠ” ì‚¬ì „ ë¡œë”©ë¨
            'statistics': {
                'tps': self.calculate_statistics(tps_values),
                'inference_time': self.calculate_statistics(inference_times),
                'run_avg_tps': self.calculate_statistics(run_avg_tps_values)
            },
            'all_runs': all_runs
        }
        
        avg_tps = self.results['ollama']['statistics']['tps']['mean']
        print(f"âœ… Ollama ì™„ë£Œ - ì „ì²´ í‰ê·  TPS: {avg_tps:.2f}")
        
    def test_llamacpp_multi_run(self):
        """llama.cpp ë‹¤ì¤‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ¦™ llama.cpp {self.num_runs}íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        all_runs = []
        tps_values = []
        inference_times = []
        
        for run_idx in range(self.num_runs):
            print(f"  ì‹¤í–‰ {run_idx + 1}/{self.num_runs}...")
            run_results = []
            
            for prompt_idx, prompt in enumerate(self.test_prompts):
                # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                full_prompt = f"{self.system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
                
                llamacpp_config = self.config['engines']['llamacpp']
                cmd = [
                    'llama-cli',
                    '-m', llamacpp_config['model_path'],
                    '-p', full_prompt,
                    '-n', str(self.max_tokens),
                    '--temp', str(self.temperature),
                    '-ngl', str(llamacpp_config['ngl']),
                    '--no-display-prompt',
                    '--chat-template', llamacpp_config['chat_template'],
                    '-st'  # single-turn mode
                ]
                
                start_time = time.time()
                try:
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=self.timeout_seconds
                    )
                    inference_time = time.time() - start_time
                    
                    if result.returncode == 0:
                        output_lines = result.stderr.split('\n')
                        tps = 0
                        
                        # ì„±ëŠ¥ ì •ë³´ íŒŒì‹± - "76.85 tokens per second" í˜•ì‹ ì°¾ê¸°
                        for line in output_lines:
                            if 'tokens per second' in line and 'eval time' in line:
                                try:
                                    # "   13.01 ms per token,    76.85 tokens per second)" í˜•ì‹ íŒŒì‹±
                                    parts = line.split('tokens per second')[0]
                                    tps = float(parts.split(',')[-1].strip())
                                    break
                                except Exception as e:
                                    print(f"      TPS íŒŒì‹± ì˜¤ë¥˜: {line} -> {e}")
                                    pass
                        
                        response = result.stdout.strip()
                        
                        # ì‘ë‹µ ìƒì„¸ ë¡œê¹…
                        self.log_response_details("llama.cpp", prompt_idx, inference_time, tps, response)
                        
                        run_results.append({
                            'prompt_idx': prompt_idx,
                            'prompt': prompt,
                            'response': response,
                            'inference_time': inference_time,
                            'tps': tps
                        })
                        
                        tps_values.append(tps)
                        inference_times.append(inference_time)
                    else:
                        print(f"    ì—ëŸ¬ (í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}): ë°˜í™˜ì½”ë“œ {result.returncode}")
                        print(f"    stderr: {result.stderr[:200]}...")
                        print(f"    stdout: {result.stdout[:200]}...")
                        
                except subprocess.TimeoutExpired:
                    print(f"    íƒ€ì„ì•„ì›ƒ (í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}) - 120ì´ˆ ì´ˆê³¼")
            
            if run_results:
                run_avg_tps = sum(r['tps'] for r in run_results) / len(run_results)
                all_runs.append({
                    'run_index': run_idx,
                    'avg_tps': run_avg_tps,
                    'tests': run_results
                })
                print(f"    ì‹¤í–‰ {run_idx + 1} í‰ê·  TPS: {run_avg_tps:.2f}")
        
        # í†µê³„ ê³„ì‚°
        run_avg_tps_values = [run['avg_tps'] for run in all_runs]
        
        self.results['llamacpp'] = {
            'engine': 'llama.cpp (GGUF + Metal)',
            'num_runs': self.num_runs,
            'load_time': 0,
            'statistics': {
                'tps': self.calculate_statistics(tps_values),
                'inference_time': self.calculate_statistics(inference_times),
                'run_avg_tps': self.calculate_statistics(run_avg_tps_values)
            },
            'all_runs': all_runs
        }
        
        avg_tps = self.results['llamacpp']['statistics']['tps']['mean']
        print(f"âœ… llama.cpp ì™„ë£Œ - ì „ì²´ í‰ê·  TPS: {avg_tps:.2f}")
        
    def test_uzu_multi_run(self):
        """Uzu ë‹¤ì¤‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (ì„œë²„ ëª¨ë“œ ì‚¬ìš©)"""
        print(f"âš¡ Uzu {self.num_runs}íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print("  Uzu ì„œë²„ ì‹œì‘ ì¤‘...")
        
        # test_uzu_only.pyì—ì„œ ê²€ì¦ëœ ë°©ì‹ ì ìš©
        import signal
        import requests
        import os
        
        # Uzu ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        uzu_config = self.config['engines']['uzu']
        
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ê²€ì¦ëœ ë°©ì‹)
        env = os.environ.copy()
        env['ROCKET_PORT'] = str(uzu_config['port'])
        
        # Uzu ì„œë²„ ì‹œì‘ (ë¡œê·¸ëŠ” ë©”ì¸ ë¡œê·¸ì— í†µí•©)
        server_process = subprocess.Popen(
            ['./uzu/target/release/uzu_cli', 'serve', uzu_config['model_path']],
            stdout=subprocess.DEVNULL,  # ì„œë²„ ì¶œë ¥ ìˆ¨ê¹€
            stderr=subprocess.DEVNULL,   # ì„œë²„ ì—ëŸ¬ ìˆ¨ê¹€
            text=True,
            env=env
        )
        
        # ì„œë²„ ë¡œê·¸ëŠ” íŒŒì¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ ë³„ë„ ëª¨ë‹ˆí„°ë§ ë¶ˆí•„ìš”
        
        # ì„œë²„ê°€ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ê²€ì¦ëœ ë°©ì‹)
        import time
        server_ready = False
        print("  ì„œë²„ ì‹œì‘ ëŒ€ê¸° ì¤‘...")
        
        for i in range(uzu_config['server_timeout']):  # ì„¤ì •ëœ ì‹œê°„ ëŒ€ê¸°
            try:
                # í¬íŠ¸ 8000ì—ì„œ í™•ì¸ (ì„œë²„ê°€ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” í¬íŠ¸)
                response = requests.get('http://localhost:8000/', timeout=2)
                print(f"  ì„œë²„ ì‘ë‹µ: {response.status_code}")
                server_ready = True
                break
            except requests.exceptions.ConnectionError:
                print(f"    ì—°ê²° ëŒ€ê¸° ì¤‘... ({i+1}/{uzu_config['server_timeout']})")
                time.sleep(1)
            except Exception as e:
                print(f"    ì˜ˆì™¸ ë°œìƒ: {e}")
                time.sleep(1)
        
        if not server_ready:
            print("  âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
            print("  ì„œë²„ í”„ë¡œì„¸ìŠ¤ ìƒíƒœ:")
            print(f"    - ë°˜í™˜ì½”ë“œ: {server_process.poll()}")
            
            # ì„œë²„ ë¡œê·¸ í™•ì¸
            try:
                stdout, stderr = server_process.communicate(timeout=5)
                print(f"    - stdout: {stdout}")
                print(f"    - stderr: {stderr}")
            except:
                print("    - ë¡œê·¸ ì½ê¸° ì‹¤íŒ¨")
            
            server_process.terminate()
            return
            
        print("  âœ… ì„œë²„ ì‹œì‘ ì„±ê³µ!")
        
        try:
            all_runs = []
            tps_values = []
            inference_times = []
            
            for run_idx in range(self.num_runs):
                print(f"  ì‹¤í–‰ {run_idx + 1}/{self.num_runs}...")
                run_results = []
                
                for prompt_idx, prompt in enumerate(self.test_prompts):
                    start_time = time.time()
                    try:
                        # OpenAI í˜¸í™˜ API í˜¸ì¶œ (êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
                        payload = {
                            "model": os.path.basename(uzu_config['model_path']),
                            "messages": [
                                {"role": "system", "content": self.system_prompt},
                                {"role": "user", "content": prompt}
                            ],
                            "max_tokens": self.max_tokens,
                            "temperature": self.temperature
                        }
                        
                        response = requests.post(
                            'http://localhost:8000/chat/completions',
                            json=payload,
                            timeout=self.timeout_seconds
                        )
                        inference_time = time.time() - start_time
                        
                        if response.status_code == 200:
                            data = response.json()
                            response_text = data['choices'][0]['message']['content']
                            
                            # TPS ê³„ì‚°
                            response_tokens = len(response_text.split()) if response_text else 0
                            tps = response_tokens / inference_time if inference_time > 0 else 0
                            
                            # ì‘ë‹µ ìƒì„¸ ë¡œê¹…
                            self.log_response_details("Uzu", prompt_idx, inference_time, tps, response_text)
                            
                            run_results.append({
                                'prompt_idx': prompt_idx,
                                'prompt': prompt,
                                'response': response_text,
                                'inference_time': inference_time,
                                'tps': tps
                            })
                            
                            tps_values.append(tps)
                            inference_times.append(inference_time)
                        else:
                            print(f"    ì—ëŸ¬ (í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}): HTTP {response.status_code}")
                            print(f"    ì‘ë‹µ: {response.text[:200]}...")
                            
                    except requests.RequestException as e:
                        print(f"    ìš”ì²­ ì—ëŸ¬ (í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}): {e}")
                    except Exception as e:
                        print(f"    ì¼ë°˜ ì—ëŸ¬ (í”„ë¡¬í”„íŠ¸ {prompt_idx + 1}): {e}")
                
                if run_results:
                    run_avg_tps = sum(r['tps'] for r in run_results) / len(run_results)
                    all_runs.append({
                        'run_index': run_idx,
                        'avg_tps': run_avg_tps,
                        'tests': run_results
                    })
                    print(f"    ì‹¤í–‰ {run_idx + 1} í‰ê·  TPS: {run_avg_tps:.2f}")
        
        finally:
            # ì„œë²„ ì¢…ë£Œ
            print("  ì„œë²„ ì¢…ë£Œ ì¤‘...")
            server_process.terminate()
            server_process.wait()
            print("  âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")
        
        # í†µê³„ ê³„ì‚°
        run_avg_tps_values = [run['avg_tps'] for run in all_runs]
        
        self.results['uzu'] = {
            'engine': 'Uzu (Native Metal)',
            'num_runs': self.num_runs,
            'load_time': 0.5,
            'statistics': {
                'tps': self.calculate_statistics(tps_values),
                'inference_time': self.calculate_statistics(inference_times),
                'run_avg_tps': self.calculate_statistics(run_avg_tps_values)
            },
            'all_runs': all_runs,
            'note': 'Simulated results based on previous benchmark'
        }
        
        avg_tps = self.results['uzu']['statistics']['tps']['mean']
        print(f"âœ… Uzu ì™„ë£Œ - ì „ì²´ í‰ê·  TPS: {avg_tps:.2f}")
        
    def generate_report(self):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        timestamp = datetime.now().isoformat()
        
        # ì½˜ì†” ì¶œë ¥
        report_header = "\n" + "="*80 + "\n"
        report_header += "ğŸ¯ Uzu AI ì¶”ë¡  ì—”ì§„ ë‹¤ì¤‘ ì‹¤í–‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ê²°ê³¼\n"
        report_header += f"ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}íšŒ, ì‹¤í–‰ ì‹œê°„: {timestamp}\n"
        report_header += "="*80
        
        print(report_header)
        
        # í†µê³„ í…Œì´ë¸”
        table_header = f"{'ì—”ì§„':<15} {'í‰ê· TPS':<10} {'TPSë²”ìœ„':<15} {'í‘œì¤€í¸ì°¨':<10} {'ìƒëŒ€ì„±ëŠ¥':<10}"
        table_separator = "-" * 75
        
        print(table_header)
        print(table_separator)
        
        baseline_tps = self.results.get('pytorch', {}).get('statistics', {}).get('tps', {}).get('mean', 1)
        
        table_rows = []
        for engine, data in self.results.items():
            engine_name = data.get('engine', engine)
            tps_stats = data.get('statistics', {}).get('tps', {})
            
            avg_tps = tps_stats.get('mean', 0)
            min_tps = tps_stats.get('min', 0)
            max_tps = tps_stats.get('max', 0)
            std_tps = tps_stats.get('std', 0)
            relative = avg_tps / baseline_tps if baseline_tps > 0 else 0
            
            tps_range = f"{min_tps:.1f}-{max_tps:.1f}"
            
            row = f"{engine_name:<15} {avg_tps:<10.2f} {tps_range:<15} {std_tps:<10.2f} {relative:<10.1f}x"
            print(row)
            table_rows.append(row)
        
        # Markdown ë¦¬í¬íŠ¸ ìƒì„±
        md_content = self._generate_markdown_report(timestamp, table_header, table_rows, baseline_tps)
        
        # Markdown íŒŒì¼ ì €ì¥ (report/ ë””ë ‰í† ë¦¬ì—)
        quick_suffix = "_quick" if self.quick_test else ""
        md_file = f'report/benchmark_report_{self.num_runs}runs{quick_suffix}_{self.timestamp}.md'
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        # ìƒì„¸ JSON ê²°ê³¼ ì €ì¥
        detailed_results = {
            'benchmark_info': {
                'timestamp': timestamp,
                'num_runs': self.num_runs,
                'num_prompts': len(self.test_prompts),
                'max_tokens': self.max_tokens,
                'test_prompts': self.test_prompts
            },
            'results': self.results,
            'summary': {}
        }
        
        # ìš”ì•½ í†µê³„ ìƒì„±
        for engine, data in self.results.items():
            detailed_results['summary'][engine] = {
                'engine_name': data.get('engine', engine),
                'avg_tps': data.get('statistics', {}).get('tps', {}).get('mean', 0),
                'tps_std': data.get('statistics', {}).get('tps', {}).get('std', 0),
                'tps_range': [
                    data.get('statistics', {}).get('tps', {}).get('min', 0),
                    data.get('statistics', {}).get('tps', {}).get('max', 0)
                ],
                'relative_performance': data.get('statistics', {}).get('tps', {}).get('mean', 0) / baseline_tps if baseline_tps > 0 else 0
            }
        
        # JSON íŒŒì¼ ì €ì¥ (output/ ë””ë ‰í† ë¦¬ì—)
        json_file = f'output/benchmark_results_{self.num_runs}runs{quick_suffix}_{self.timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š ìƒì„¸ ê²°ê³¼ê°€ {json_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“‹ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ê°€ {md_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ˆ ì´ {sum(len(data.get('all_runs', [])) for data in self.results.values())}íšŒì˜ ê°œë³„ ì‹¤í–‰ ê²°ê³¼ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ìƒì„¸ ë¡œê·¸: {self.log_filename}")
        
    def _get_system_info(self) -> Dict[str, str]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        import platform
        import subprocess
        
        system_info = {}
        
        try:
            # macOS í•˜ë“œì›¨ì–´ ì •ë³´
            if platform.system() == "Darwin":
                result = subprocess.run(
                    ['system_profiler', 'SPHardwareDataType'],
                    capture_output=True, text=True, timeout=10
                )
                if result.returncode == 0:
                    lines = result.stdout.split('\n')
                    for line in lines:
                        if 'Model Name:' in line:
                            system_info['model'] = line.split('Model Name:')[1].strip()
                        elif 'Model Identifier:' in line:
                            system_info['model_id'] = line.split('Model Identifier:')[1].strip()
                        elif 'Chip:' in line:
                            system_info['processor'] = line.split('Chip:')[1].strip()
                        elif 'Total Number of Cores:' in line:
                            system_info['cores'] = line.split('Total Number of Cores:')[1].strip()
                        elif 'Memory:' in line:
                            system_info['memory'] = line.split('Memory:')[1].strip()
            
            # OS ë²„ì „
            result = subprocess.run(['sw_vers'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                for line in lines:
                    if 'ProductVersion:' in line:
                        system_info['os_version'] = line.split('ProductVersion:')[1].strip()
                    elif 'BuildVersion:' in line:
                        system_info['build_version'] = line.split('BuildVersion:')[1].strip()
            
            # Python ë²„ì „
            import sys
            system_info['python_version'] = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
            
        except Exception as e:
            print(f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return system_info
        
    def _generate_markdown_report(self, timestamp: str, table_header: str, table_rows: List[str], baseline_tps: float) -> str:
        """Markdown ë¦¬í¬íŠ¸ ìƒì„±"""
        system_info = self._get_system_info()
        
        md_content = f"""# Uzu AI ì¶”ë¡  ì—”ì§„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

## ì‹œìŠ¤í…œ ì •ë³´
- **ëª¨ë¸**: {system_info.get('model', 'Unknown')} ({system_info.get('model_id', 'Unknown')})
- **í”„ë¡œì„¸ì„œ**: {system_info.get('processor', 'Unknown')}
- **CPU ì½”ì–´**: {system_info.get('cores', 'Unknown')}
- **ë©”ëª¨ë¦¬**: {system_info.get('memory', 'Unknown')}
- **ìš´ì˜ì²´ì œ**: macOS {system_info.get('os_version', 'Unknown')} ({system_info.get('build_version', 'Unknown')})
- **Python**: {system_info.get('python_version', 'Unknown')}

## ë²¤ì¹˜ë§ˆí¬ ì •ë³´
- **ì‹¤í–‰ ì‹œê°„**: {timestamp}
- **ë°˜ë³µ íšŸìˆ˜**: {self.num_runs}íšŒ
- **í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸**: {len(self.test_prompts)}ê°œ
- **ìµœëŒ€ í† í°**: {self.max_tokens}ê°œ
- **ì´ ì‹¤í–‰ íšŸìˆ˜**: {len(self.test_prompts) * self.num_runs * len(self.results)}íšŒ

## ì„±ëŠ¥ ìš”ì•½

| ì—”ì§„ | í‰ê·  TPS | TPSë²”ìœ„ | í‘œì¤€í¸ì°¨ | ìƒëŒ€ ì„±ëŠ¥ |
|------|----------|----------|----------|----------|
"""
        
        # í…Œì´ë¸” ë°ì´í„° ë³€í™˜
        for engine, data in self.results.items():
            engine_name = data.get('engine', engine)
            tps_stats = data.get('statistics', {}).get('tps', {})
            
            avg_tps = tps_stats.get('mean', 0)
            min_tps = tps_stats.get('min', 0)
            max_tps = tps_stats.get('max', 0)
            std_tps = tps_stats.get('std', 0)
            relative = avg_tps / baseline_tps if baseline_tps > 0 else 0
            
            tps_range = f"{min_tps:.1f}-{max_tps:.1f}"
            
            md_content += f"| {engine_name} | {avg_tps:.2f} | {tps_range} | {std_tps:.2f} | {relative:.1f}x |\n"
        
        # ìƒì„¸ í†µê³„ ì¶”ê°€
        md_content += "\n## ìƒì„¸ í†µê³„\n\n"
        
        for engine, data in self.results.items():
            engine_name = data.get('engine', engine)
            tps_stats = data.get('statistics', {}).get('tps', {})
            inference_stats = data.get('statistics', {}).get('inference_time', {})
            
            md_content += f"### {engine_name}\n\n"
            md_content += f"**TPS (Tokens Per Second)**\n"
            md_content += f"- í‰ê· : {tps_stats.get('mean', 0):.2f}\n"
            md_content += f"- ì¤‘ê°„ê°’: {tps_stats.get('median', 0):.2f}\n"
            md_content += f"- ìµœì†Œê°’: {tps_stats.get('min', 0):.2f}\n"
            md_content += f"- ìµœëŒ€ê°’: {tps_stats.get('max', 0):.2f}\n"
            md_content += f"- í‘œì¤€í¸ì°¨: {tps_stats.get('std', 0):.2f}\n\n"
            
            md_content += f"**ì¶”ë¡  ì‹œê°„ (ì´ˆ)**\n"
            md_content += f"- í‰ê· : {inference_stats.get('mean', 0):.3f}\n"
            md_content += f"- ì¤‘ê°„ê°’: {inference_stats.get('median', 0):.3f}\n"
            md_content += f"- ìµœì†Œê°’: {inference_stats.get('min', 0):.3f}\n"
            md_content += f"- ìµœëŒ€ê°’: {inference_stats.get('max', 0):.3f}\n"
            md_content += f"- í‘œì¤€í¸ì°¨: {inference_stats.get('std', 0):.3f}\n\n"
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ëª©ë¡ ì¶”ê°€
        md_content += "## í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸\n\n"
        for i, prompt in enumerate(self.test_prompts, 1):
            md_content += f"{i}. \"{prompt}\"\n"
        
        # ì‹¤í–‰ë³„ ê²°ê³¼ ìš”ì•½
        md_content += "\n## ì‹¤í–‰ë³„ í‰ê·  TPS\n\n"
        for engine, data in self.results.items():
            engine_name = data.get('engine', engine)
            md_content += f"### {engine_name}\n\n"
            
            all_runs = data.get('all_runs', [])
            if all_runs:
                md_content += "| ì‹¤í–‰ | í‰ê·  TPS |\n"
                md_content += "|------|----------|\n"
                for run in all_runs:
                    run_idx = run.get('run_index', 0) + 1
                    avg_tps = run.get('avg_tps', 0)
                    md_content += f"| {run_idx} | {avg_tps:.2f} |\n"
                md_content += "\n"
        
        # ê²°ë¡ 
        md_content += "## ê²°ë¡ \n\n"
        
        # ì„±ëŠ¥ ìˆœìœ„
        sorted_engines = sorted(
            [(engine, data.get('statistics', {}).get('tps', {}).get('mean', 0)) 
             for engine, data in self.results.items()], 
            key=lambda x: x[1], 
            reverse=True
        )
        
        md_content += "**ì„±ëŠ¥ ìˆœìœ„ (í‰ê·  TPS ê¸°ì¤€)**\n\n"
        for rank, (engine, avg_tps) in enumerate(sorted_engines, 1):
            engine_name = self.results[engine].get('engine', engine)
            relative = avg_tps / baseline_tps if baseline_tps > 0 else 0
            md_content += f"{rank}. **{engine_name}**: {avg_tps:.2f} TPS ({relative:.1f}x)\n"
        
        # ì•ˆì •ì„± ë¶„ì„
        md_content += "\n**ì„±ëŠ¥ ì•ˆì •ì„± (í‘œì¤€í¸ì°¨ ê¸°ì¤€)**\n\n"
        sorted_stability = sorted(
            [(engine, data.get('statistics', {}).get('tps', {}).get('std', 0)) 
             for engine, data in self.results.items()], 
            key=lambda x: x[1]
        )
        
        for rank, (engine, std) in enumerate(sorted_stability, 1):
            engine_name = self.results[engine].get('engine', engine)
            md_content += f"{rank}. **{engine_name}**: í‘œì¤€í¸ì°¨ {std:.2f} TPS\n"
        
        quick_suffix = "_quick" if self.quick_test else ""
        md_content += f"\n---\n\n*ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œê°„: {timestamp}*\n"
        md_content += f"*JSON ë°ì´í„°: output/benchmark_results_{self.num_runs}runs{quick_suffix}_{self.timestamp}.json*\n"
        md_content += f"*ìƒì„¸ ë¡œê·¸: {self.log_filename}*\n"
        
        return md_content
        
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info(f"ğŸš€ Uzu AI ì¶”ë¡  ì—”ì§„ {self.num_runs}íšŒ ë°˜ë³µ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        self.logger.info(f"í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(self.test_prompts)}")
        self.logger.info(f"ìµœëŒ€ í† í° ìˆ˜: {self.max_tokens} (êµ¬ì¡°í™”ëœ ì‘ë‹µì— ì¶©ë¶„í•œ ê¸¸ì´)")
        self.logger.info(f"ì˜¨ë„ ì„¤ì •: {self.temperature} (ì¼ê´€ëœ ì‘ë‹µ)")
        
        # í™œì„±í™”ëœ ì—”ì§„ë§Œ ì¹´ìš´íŠ¸
        enabled_engines = [name for name, config in self.config['engines'].items() if config.get('enabled', True)]
        self.logger.info(f"í™œì„±í™”ëœ ì—”ì§„: {', '.join(enabled_engines)}")
        self.logger.info(f"ì´ ì˜ˆìƒ ì‹¤í–‰ íšŸìˆ˜: {len(self.test_prompts) * self.num_runs * len(enabled_engines)}íšŒ")
        self.logger.info(f"ìƒì„¸ ë¡œê·¸ íŒŒì¼: {self.log_filename}")
        self.logger.info("")
        
        engines_to_test = [
            ('pytorch', self.test_pytorch_mps_multi_run),
            ('ollama', self.test_ollama_multi_run),
            ('llamacpp', self.test_llamacpp_multi_run),
            ('uzu', self.test_uzu_multi_run)
        ]
        
        for engine_name, test_method in engines_to_test:
            # ì„¤ì •ì—ì„œ ì—”ì§„ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not self.config['engines'].get(engine_name, {}).get('enabled', True):
                print(f"â­ï¸  {engine_name} ì—”ì§„ ë¹„í™œì„±í™”ë¨, ê±´ë„ˆë›°ê¸°")
                continue
                
            try:
                test_method()
                print()
            except Exception as e:
                print(f"âŒ {engine_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                self.logger.error(f"{engine_name} í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}", exc_info=True)
                print()
        
        self.generate_report()


if __name__ == "__main__":
    import sys
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
    os.chdir(os.path.dirname(os.path.abspath(__file__)))
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    num_runs = None
    quick_test = False
    
    # ì¸ì íŒŒì‹±
    args = sys.argv[1:]
    for arg in args:
        if arg.lower() in ('quick', 'q', '--quick', '-q'):
            quick_test = True
        else:
            try:
                num_runs = int(arg)
                if num_runs < 1:
                    raise ValueError("ì‹¤í–‰ íšŸìˆ˜ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            except ValueError:
                print(f"ì˜¤ë¥˜: ì˜ëª»ëœ ì¸ì '{arg}'")
                print("ì‚¬ìš©ë²•: python bench_run.py [ì‹¤í–‰íšŸìˆ˜] [quick]")
                print("ì˜ˆì‹œ:")
                print("  python bench_run.py 5        # 5íšŒ ë°˜ë³µ, ì „ì²´ í”„ë¡¬í”„íŠ¸")
                print("  python bench_run.py 1 quick  # 1íšŒ ë°˜ë³µ, í”„ë¡¬í”„íŠ¸ 1ê°œë§Œ")
                print("  python bench_run.py quick    # ê¸°ë³¸ ë°˜ë³µ, í”„ë¡¬í”„íŠ¸ 1ê°œë§Œ")
                sys.exit(1)
    
    # MultiRunBenchmarkRunnerì—ì„œ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê²°ì •ë¨:
    # 1. ëª…ë ¹í–‰ ì¸ì (num_runs) > 2. í™˜ê²½ë³€ìˆ˜ > 3. ì„¤ì • íŒŒì¼ > 4. ê¸°ë³¸ê°’
    runner = MultiRunBenchmarkRunner(num_runs=num_runs, quick_test=quick_test)
    actual_runs = runner.num_runs
    
    test_mode = "ë¹ ë¥¸ í…ŒìŠ¤íŠ¸" if quick_test else "ì •ì‹ ë²¤ì¹˜ë§ˆí¬"
    print(f"{actual_runs}íšŒ ë°˜ë³µ {test_mode}ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
    
    if num_runs:
        print(f"  (ëª…ë ¹í–‰ ì¸ìë¡œ ì„¤ì •ë¨)")
    elif os.getenv('BENCHMARK_NUM_RUNS'):
        print(f"  (í™˜ê²½ë³€ìˆ˜ BENCHMARK_NUM_RUNS={os.getenv('BENCHMARK_NUM_RUNS')})")
    else:
        print(f"  (ì„¤ì • íŒŒì¼ ê¸°ë³¸ê°’)")
    
    if quick_test:
        print(f"  âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í”„ë¡¬í”„íŠ¸ 1ê°œë§Œ ì‚¬ìš©")
    
    print(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ êµ¬ì„±:")
    print(f"  - í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(runner.test_prompts)}ê°œ")
    print(f"  - í™œì„±í™”ëœ ì—”ì§„: {len([name for name, config in runner.config['engines'].items() if config.get('enabled', True)])}ê°œ")
    print(f"  - ê° ì—”ì§„ë‹¹ ì‹¤í–‰: {actual_runs}íšŒ")
    print(f"  - ì´ ì‹¤í–‰ íšŸìˆ˜: {len(runner.test_prompts)} Ã— {actual_runs} Ã— {len([name for name, config in runner.config['engines'].items() if config.get('enabled', True)])} = {len(runner.test_prompts) * actual_runs * len([name for name, config in runner.config['engines'].items() if config.get('enabled', True)])}íšŒ")
    print()
    runner.run_all_tests() 