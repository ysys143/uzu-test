# Uzu AI ì¶”ë¡  ì—”ì§„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

Apple Silicon (M3 Max)ì—ì„œ ë‹¤ì–‘í•œ AI ì¶”ë¡  ì—”ì§„ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“Š í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ**: Uzu, PyTorch, Ollama, llama.cppì˜ Apple Silicon ìµœì í™” ì„±ëŠ¥ ë¹„êµ  
**í…ŒìŠ¤íŠ¸ ëª¨ë¸**: Google Gemma-3-1B-IT  
**í…ŒìŠ¤íŠ¸ í™˜ê²½**: macOS (Apple Silicon M3 Max)  
**ì¸¡ì • ì§€í‘œ**: TPS (Tokens Per Second), ì¶”ë¡  ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

## ğŸ—ï¸ ì„œë¹™ ë°©ì‹ ê°œìš”

### 1. CLI ê¸°ë°˜ ì„œë¹™ (Subprocess ë²¤ì¹˜ë§ˆí¬)

ìˆœìˆ˜í•œ CLI ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ê° ì—”ì§„ì˜ ìµœì í™”ëœ ëª…ë ¹í–‰ ë„êµ¬ë¥¼ ì§ì ‘ ì‹¤í–‰í•©ë‹ˆë‹¤.

1. **PyTorch + MPS**: HuggingFace Transformers + Apple Metal Performance Shaders
2. **Ollama CLI**: GGUF ëª¨ë¸ì„ í†µí•œ ëŒ€í™”í˜• ì¶”ë¡ 
3. **llama.cpp CLI**: Metal ê°€ì† í™œìš©í•œ ì§ì ‘ ì¶”ë¡ 

### 2. API ê¸°ë°˜ ì„œë¹™ (API ë²¤ì¹˜ë§ˆí¬)

HTTP APIë¥¼ í†µí•œ ì„œë²„ ëª¨ë“œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

4. **PyTorch Server**: FastAPI ê¸°ë°˜ OpenAI í˜¸í™˜ ì„œë²„
5. **Ollama Server**: ë‚´ì¥ HTTP ì„œë²„ ëª¨ë“œ
6. **llama.cpp Server**: llama-serverë¥¼ í†µí•œ HTTP API
7. **Uzu Server**: Rust ê¸°ë°˜ ë„¤ì´í‹°ë¸Œ Metal ì„œë²„

## ğŸ› ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **ìš´ì˜ì²´ì œ**: macOS 12.0+ (Apple Silicon ê¶Œì¥)
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 16GB (32GB+ ê¶Œì¥)
- **Python**: 3.9+
- **Rust**: 1.86.0+
- **Xcode Command Line Tools**: Uzu Metal ì»´íŒŒì¼ì„ ìœ„í•´ í•„ìˆ˜
- **Xcode**: 15.0+ (ë˜ëŠ” Command Line Tools for Xcode)

## ğŸ“¦ ì˜ì¡´ì„± ì„¤ì¹˜

### 1. ê¸°ë³¸ í™˜ê²½ ì„¤ì •

```bash
# benchmark ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd benchmark

# Python ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
uv venv
source .venv/bin/activate

# ë²¤ì¹˜ë§ˆí¬ ì˜ì¡´ì„± ì„¤ì¹˜
uv pip install -e .

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ë³µê·€
cd ..

# ì´í›„ ëª¨ë“  Python ëª…ë ¹ì€ benchmark/.venv í™˜ê²½ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤
```

**ì¤‘ìš”**: ê°€ìƒí™˜ê²½ì€ `benchmark/.venv`ì— ìƒì„±ë˜ë©°, í„°ë¯¸ë„ì„ ìƒˆë¡œ ì—´ ë•Œë§ˆë‹¤ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í™œì„±í™”í•´ì•¼ í•©ë‹ˆë‹¤:
```bash
cd benchmark && source .venv/bin/activate && cd ..
```

### 2. AI ì¶”ë¡  ì—”ì§„ ì„¤ì¹˜

#### PyTorch (MPS ë°±ì—”ë“œ)
```bash
# PyTorch with Metal Performance Shaders (benchmark/pyproject.tomlì— í¬í•¨ë¨)
# ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš” - ë²¤ì¹˜ë§ˆí¬ ì˜ì¡´ì„±ì—ì„œ ìë™ ì„¤ì¹˜ë¨

# ìˆ˜ë™ ì„¤ì¹˜ê°€ í•„ìš”í•œ ê²½ìš°:
# uv pip install torch torchvision torchaudio transformers accelerate
```

#### Ollama
```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ì„œë¹„ìŠ¤ í™•ì¸
ollama --version  # v0.9.3+
```

#### llama.cpp (Metal ì§€ì›)
```bash
# llama.cpp í´ë¡  ë° ë¹Œë“œ
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Metal ì§€ì›ìœ¼ë¡œ ë¹Œë“œ
make GGML_METAL=1

# ì‹¤í–‰ íŒŒì¼ PATH ì¶”ê°€
export PATH="$PWD:$PATH"
```

#### Uzu ì—”ì§„
```bash
# 1. Xcode Command Line Tools ì„¤ì¹˜ (Metal ì»´íŒŒì¼ì„ ìœ„í•´ í•„ìˆ˜)
xcode-select --install

# Xcode ë¼ì´ì„¼ìŠ¤ ë™ì˜ (ë¹Œë“œ ì˜¤ë¥˜ ë°©ì§€)
sudo xcodebuild -license accept

# 2. Uzu í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-org/uzu.git
cd uzu/uzu

# 3. Rust ë¹Œë“œ (Release ëª¨ë“œ)
# Metal ì…°ì´ë” ì»´íŒŒì¼ì„ ìœ„í•´ Xcode íˆ´ì²´ì¸ ì‚¬ìš©
cargo build --release

# 4. CLI ë„êµ¬ í™•ì¸
./target/release/uzu_cli --help

# 5. Metal ì§€ì› í™•ì¸
./target/release/uzu_cli --version
```

**Xcode ì˜ì¡´ì„± ì´ìœ :**
- **Metal ì…°ì´ë” ì»´íŒŒì¼**: Uzuì˜ Metal ì»¤ë„(.metal íŒŒì¼) ì»´íŒŒì¼ì— í•„ìš”
- **Apple ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬**: Metal Performance Shaders ë“± Apple ë„¤ì´í‹°ë¸Œ API ì‚¬ìš©
- **ë¹Œë“œ íˆ´ì²´ì¸**: Rustì—ì„œ Metal ì½”ë“œë¥¼ ë¹Œë“œí•˜ê¸° ìœ„í•œ Apple ì»´íŒŒì¼ëŸ¬ í•„ìš”

#### Lalamo (ëª¨ë¸ ë³€í™˜ ë„êµ¬)
```bash
# Lalamo ì„¤ì¹˜ (ëª¨ë¸ ë³€í™˜ìš©)
git clone https://github.com/your-org/lalamo.git
cd lalamo
uv pip install -e .
```

## ğŸ¯ ë²¤ì¹˜ë§ˆí¬ ê³µì •ì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ í†µì¼

### êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ

ë²¤ì¹˜ë§ˆí¬ì˜ ê³µì •ì„±ê³¼ ì¼ê´€ì„±ì„ ìœ„í•´ ëª¨ë“  ì—”ì§„ì—ì„œ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

#### 1. í†µì¼ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

```python
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‹µë³€ (1-2ë¬¸ì¥)
2. ìƒì„¸ ì„¤ëª… (2-3ë¬¸ì¥)  
3. ì¶”ê°€ ì •ë³´ë‚˜ íŒ (1-2ë¬¸ì¥)

**ì¤‘ìš”: ë‹µë³€ì€ ë°˜ë“œì‹œ 300ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.**
í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
```

#### 2. ê³„ì¸µí™”ëœ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸

ì‘ë‹µ ê¸¸ì´ì™€ ë³µì¡ë„ì— ë”°ë¼ 3ë‹¨ê³„ë¡œ êµ¬ë¶„ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

- **ì§§ì€ í”„ë¡¬í”„íŠ¸** (30-80 í† í°): "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?"
- **ì¤‘ê°„ í”„ë¡¬í”„íŠ¸** (80-150 í† í°): "ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì‹œ ì •ê·œí™”ì˜ ê°œë…ê³¼ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
- **ê¸´ í”„ë¡¬í”„íŠ¸** (150+ í† í°): "ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ ì¼ê´€ì„±, ê°€ìš©ì„±, ë¶„í•  í—ˆìš©ì„± ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì„¤ëª…í•˜ê³ ..."

#### 3. ì±„íŒ… í…œí”Œë¦¿ í†µì¼í™” ê³¼ì •

ê° ì—”ì§„ì´ ì‚¬ìš©í•˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì±„íŒ… í…œí”Œë¦¿ì„ **Gemma í‘œì¤€ í˜•ì‹**ìœ¼ë¡œ í†µì¼í–ˆìŠµë‹ˆë‹¤:

##### Gemma í‘œì¤€ ì±„íŒ… í…œí”Œë¦¿
```xml
<start_of_turn>system
{ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸}
<end_of_turn>
<start_of_turn>user
{ì‚¬ìš©ì ì§ˆë¬¸}
<end_of_turn>
<start_of_turn>model
{AI ì‘ë‹µ}
<end_of_turn>
```

##### ì—”ì§„ë³„ í…œí”Œë¦¿ í†µì¼ ê³¼ì •

**1. PyTorch (HuggingFace Transformers)**
```python
# ìë™ í…œí”Œë¦¿ ì ìš© (ë‚´ì¥ Gemma í…œí”Œë¦¿ ì‚¬ìš©)
messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": prompt}
]
formatted_prompt = tokenizer.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
# ê²°ê³¼: "<start_of_turn>system\n{system}<end_of_turn>\n<start_of_turn>user\n{user}<end_of_turn>\n<start_of_turn>model\n"
```

**2. Ollama (Modelfile í…œí”Œë¦¿ ì„¤ì •)**
```bash
# models/Modelfileì—ì„œ Gemma í…œí”Œë¦¿ ëª…ì‹œì  ì •ì˜
TEMPLATE """{{ if .System }}<start_of_turn>system
{{ .System }}<end_of_turn>
{{ end }}{{ if .Prompt }}<start_of_turn>user
{{ .Prompt }}<end_of_turn>
{{ end }}<start_of_turn>model
{{ .Response }}<end_of_turn>
"""
PARAMETER stop "<start_of_turn>"
PARAMETER stop "<end_of_turn>"
```

**3. llama.cpp (ëª…ë ¹í–‰ ì˜µì…˜ ì„¤ì •)**
```bash
# --chat-template gemma ì˜µì…˜ìœ¼ë¡œ Gemma í…œí”Œë¦¿ ê°•ì œ ì ìš©
llama-server \
    --model ./models/gemma-3-1b-it-gguf-llama/model.gguf \
    --chat-template gemma \
    --host 127.0.0.1 --port 8002
```

**4. Uzu (OpenAI í˜¸í™˜ API)**
```python
# OpenAI í˜¸í™˜ ë©”ì‹œì§€ í˜•ì‹ (ë‚´ë¶€ì ìœ¼ë¡œ Gemma í…œí”Œë¦¿ ì ìš©)
payload = {
    "messages": [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt}
    ]
}
# Uzu ë‚´ë¶€ì—ì„œ Gemma í˜•ì‹ìœ¼ë¡œ ìë™ ë³€í™˜
```

##### í…œí”Œë¦¿ í†µì¼ì˜ ì¤‘ìš”ì„±

ì±„íŒ… í…œí”Œë¦¿ì´ ë‹¤ë¥´ë©´ **ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¼ë„ ì™„ì „íˆ ë‹¤ë¥¸ ê²°ê³¼**ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ì˜ëª»ëœ ê²½ìš° (í…œí”Œë¦¿ ë¶ˆì¼ì¹˜)
llama.cpp: "User: ì•ˆë…•í•˜ì„¸ìš”\nAssistant:"           # ì„±ëŠ¥ ì €í•˜
PyTorch:   "<start_of_turn>user\nì•ˆë…•í•˜ì„¸ìš”<end_of_turn><start_of_turn>model\n"  # ì •ìƒ

# ì˜¬ë°”ë¥¸ ê²½ìš° (Gemma í…œí”Œë¦¿ í†µì¼)
ëª¨ë“  ì—”ì§„: "<start_of_turn>system\n{system}<end_of_turn><start_of_turn>user\nì•ˆë…•í•˜ì„¸ìš”<end_of_turn><start_of_turn>model\n"
```

ì´ í†µì¼í™”ë¥¼ í†µí•´ **ìˆœìˆ˜í•œ ì¶”ë¡  ì—”ì§„ ì„±ëŠ¥**ë§Œì„ ë¹„êµí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

#### 4. ì‘ë‹µ ì¼ê´€ì„± ë³´ì¥

- **í† í° ì œí•œ**: ëª¨ë“  ì—”ì§„ì—ì„œ max_tokens=500ìœ¼ë¡œ í†µì¼
- **ì˜¨ë„ ì„¤ì •**: temperature=0.3ìœ¼ë¡œ ì¼ê´€ëœ ì°½ì˜ì„± ìˆ˜ì¤€ ìœ ì§€
- **ê¸¸ì´ ì œí•œ**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ 300ì ì´ë‚´ ì‘ë‹µ ìœ ë„
- **ì–¸ì–´ í†µì¼**: ëª¨ë“  ì‘ë‹µì„ í•œêµ­ì–´ë¡œ ì œí•œ

ì´ëŸ¬í•œ í†µì¼í™”ë¥¼ í†µí•´ ê° ì—”ì§„ì˜ ìˆœìˆ˜í•œ ì¶”ë¡  ì„±ëŠ¥ë§Œì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¤– ëª¨ë¸ ì¤€ë¹„ ë° ë³€í™˜

### 1. HuggingFace ì›ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p models

# HuggingFace Hubì—ì„œ Gemma-3-1B-IT ë‹¤ìš´ë¡œë“œ
uv pip install huggingface_hub
python -c "
from huggingface_hub import snapshot_download
snapshot_download('google/gemma-3-1b-it', local_dir='./models/gemma-3-1b-it')
"
```

### 2. ì—”ì§„ë³„ ëª¨ë¸ í˜•ì‹ ë³€í™˜

#### ëª¨ë¸ í˜•ì‹ ì§€ì› í˜„í™©

| ì—”ì§„ | ì§€ì› í˜•ì‹ | ë³€í™˜ í•„ìš” | ì„¤ëª… |
|------|-----------|-----------|------|
| **PyTorch** | SafeTensors | âŒ | HuggingFace ì›ë³¸ ì§ì ‘ ì‚¬ìš© |
| **Uzu** | SafeTensors | âŒ | HuggingFace ì›ë³¸ ì§ì ‘ ì‚¬ìš© |
| **Ollama** | SafeTensors, GGUF | âŒ | ë„¤ì´í‹°ë¸Œ SafeTensors ì§€ì› |
| **llama.cpp** | GGUF | âœ… | GGUF ë³€í™˜ í•„ìš” |

#### PyTorch (ë³€í™˜ ë¶ˆí•„ìš”)
```bash
# HuggingFace ì›ë³¸ ëª¨ë¸ ì§ì ‘ ì‚¬ìš©
# ./models/gemma-3-1b-it/ ê·¸ëŒ€ë¡œ ì‚¬ìš©
```

#### Uzu (ë³€í™˜ ë¶ˆí•„ìš”)
```bash
# HuggingFace SafeTensors í˜•ì‹ ì§ì ‘ ì§€ì›
cp -r ./models/gemma-3-1b-it ./models/gemma-3-1b-it-uzu
```

#### Ollama (SafeTensors ì§ì ‘ ì‚¬ìš©)
```bash
# OllamaëŠ” HuggingFace SafeTensorsë¥¼ ì§ì ‘ ì§€ì›
# ë³„ë„ ë³€í™˜ ì—†ì´ ì›ë³¸ ëª¨ë¸ì„ ë°”ë¡œ ì‚¬ìš©

# 1. Ollama Modelfile ìƒì„±
cat > ./models/Modelfile << 'EOF'
FROM ./gemma-3-1b-it
TEMPLATE """{{ if .System }}<start_of_turn>system
{{ .System }}<end_of_turn>
{{ end }}{{ if .Prompt }}<start_of_turn>user
{{ .Prompt }}<end_of_turn>
{{ end }}<start_of_turn>model
{{ .Response }}<end_of_turn>
"""
PARAMETER stop "<start_of_turn>"
PARAMETER stop "<end_of_turn>"
EOF

# 2. Ollamaì— ëª¨ë¸ ë“±ë¡
cd models
ollama create gemma-3-1b-it-bench -f Modelfile
ollama list  # ë“±ë¡ í™•ì¸

# 3. ëª¨ë¸ ì •ë³´ í™•ì¸
ollama show gemma-3-1b-it-bench
```

#### llama.cpp (GGUF ë³€í™˜)
```bash
# HuggingFace â†’ GGUF ë³€í™˜ (llama.cppëŠ” GGUF í˜•ì‹ í•„ìš”)
python llama.cpp/convert_hf_to_gguf.py \
    ./models/gemma-3-1b-it/ \
    --outfile ./models/gemma-3-1b-it-gguf-llama/model.gguf \
    --outtype f16

# Metal ì§€ì› í™•ì¸
llama-cli --help | grep -i metal
```

## âš™ï¸ ë²¤ì¹˜ë§ˆí¬ ì„¤ì •

### ì„¤ì • íŒŒì¼ êµ¬ì¡°

ë²¤ì¹˜ë§ˆí¬ëŠ” `benchmark_config.json` íŒŒì¼ë¡œ í†µí•© ê´€ë¦¬ë©ë‹ˆë‹¤:

```json
{
  "benchmark": {
    "max_tokens": 500,
    "temperature": 0.3,
    "num_runs": 10,
    "timeout_seconds": 120,
    "system_prompt_override": null
  },
  "engines": {
    "pytorch": {
      "enabled": true,
      "device": "mps",
      "torch_dtype": "float16"
    },
    "ollama": {
      "enabled": true,
      "model_name": "gemma-3-1b-it-bench",
      "verbose": true
    },
    "llamacpp": {
      "enabled": true,
      "model_path": "./models/gemma-3-1b-it-gguf-llama/model.gguf",
      "ngl": 99,
      "chat_template": "gemma"
    },
    "uzu": {
      "enabled": false,
      "model_path": "./models/gemma-3-1b-it-uzu",
      "port": 51839,
      "server_timeout": 60,
      "note": "CLI ëª¨ë“œëŠ” ëŒ€í™”í˜• ì „ìš©ìœ¼ë¡œ ìŠ¤í¬ë¦½íŠ¸ ìë™í™” ë¶ˆê°€ëŠ¥"
    }
  },
  "servers": {
    "pytorch": {
      "enabled": true,
      "port": 8001,
      "model_path": "./models/gemma-3-1b-it",
      "startup_timeout": 60,
      "api_endpoint": "/chat/completions"
    },
    "ollama": {
      "enabled": true,
      "port": 11434,
      "model_name": "gemma-3-1b-it-bench",
      "startup_timeout": 30,
      "api_endpoint": "/api/generate"
    },
    "llamacpp": {
      "enabled": true,
      "port": 8002,
      "model_path": "./models/gemma-3-1b-it-gguf-llama/model.gguf",
      "startup_timeout": 30,
      "api_endpoint": "/completion",
      "ngl": 99,
      "chat_template": "gemma"
    },
    "uzu": {
      "enabled": true,
      "port": 8000,
      "model_path": "./models/gemma-3-1b-it-uzu",
      "startup_timeout": 60,
      "api_endpoint": "/chat/completions"
    }
  }
}
```

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# ë²¤ì¹˜ë§ˆí¬ ë§¤ê°œë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ
export BENCHMARK_MAX_TOKENS=300
export BENCHMARK_TEMPERATURE=0.1
export BENCHMARK_NUM_RUNS=5
export BENCHMARK_TIMEOUT=90

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í…€
export BENCHMARK_SYSTEM_PROMPT="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
```

## ğŸš€ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

### 1. CLI ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ (Subprocess ë°©ì‹)

ìˆœìˆ˜í•œ ê° ì—”ì§„ì˜ CLI ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤:

```bash
# ì •ì‹ ë²¤ì¹˜ë§ˆí¬ (10íšŒ ë°˜ë³µ, 10ê°œ í”„ë¡¬í”„íŠ¸)
python3 subprocess_benchmark.py

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (1íšŒ ë°˜ë³µ, 1ê°œ í”„ë¡¬í”„íŠ¸)
python3 subprocess_benchmark.py 1 quick

# ì»¤ìŠ¤í…€ ë°˜ë³µ íšŸìˆ˜
python3 subprocess_benchmark.py 5
```

**ì§€ì› ì—”ì§„**: PyTorch, Ollama, llama.cpp  
**ì œì™¸ ì—”ì§„**: Uzu (ëŒ€í™”í˜• CLIë¡œ ìŠ¤í¬ë¦½íŠ¸ ìë™í™” ë¶ˆê°€)

### 2. API ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ (ì„œë²„ ë°©ì‹)

ëª¨ë“  ì—”ì§„ì„ HTTP ì„œë²„ë¡œ ì‹¤í–‰í•˜ì—¬ API ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤:

```bash
# ì •ì‹ ë²¤ì¹˜ë§ˆí¬ (ëª¨ë“  ì„œë²„ ìë™ ì‹œì‘/ì¢…ë£Œ)
python3 api_benchmark.py

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python3 api_benchmark.py quick

# ì»¤ìŠ¤í…€ ì„¤ì •
export BENCHMARK_NUM_RUNS=3
python3 api_benchmark.py
```

**ì§€ì› ì—”ì§„**: PyTorch Server, Ollama Server, llama.cpp Server, Uzu Server

### 3. ê°œë³„ ì„œë²„ ê´€ë¦¬

```bash
# ì„œë²„ ê´€ë¦¬ìë¥¼ í†µí•œ ìˆ˜ë™ ì œì–´
python3 server_manager.py

# ê°œë³„ ì„œë²„ ì‹¤í–‰
python3 pytorch_server.py --port 8001 --model-path ./models/gemma-3-1b-it
```

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„

### ìµœì‹  ì„±ëŠ¥ ê²°ê³¼ (2025-07-18)

#### CLI ëª¨ë“œ ì„±ëŠ¥ (Subprocess)
| ì—”ì§„ | í‰ê·  TPS | ìƒëŒ€ ì„±ëŠ¥ | íŠ¹ì§• |
|------|----------|----------|------|
| PyTorch + MPS | 7.31 | 1.0x (ê¸°ì¤€) | ë†’ì€ í’ˆì§ˆ, ëŠë¦° ì†ë„ |
| Ollama (GGUF) | 74.49 | 10.2x | ê· í˜•ì¡íŒ ì„±ëŠ¥ |
| llama.cpp (Metal) | 2,337.30 | 319.5x | ì••ë„ì  ì†ë„ |

#### API ëª¨ë“œ ì„±ëŠ¥ (Server)
| ì—”ì§„ | í‰ê·  TPS | ìƒëŒ€ ì„±ëŠ¥ | íŠ¹ì§• |
|------|----------|----------|------|
| PyTorch Server | 7.27 | 1.0x (ê¸°ì¤€) | OpenAI í˜¸í™˜ API |
| Ollama Server | 46.59 | 6.4x | ë©€í‹°í´ë¼ì´ì–¸íŠ¸ ì§€ì› |
| llama.cpp Server | 71.52 | 9.8x | HTTP ì˜¤ë²„í—¤ë“œ ì¡´ì¬ |
| Uzu Server | 26.78 | 3.7x | ë„¤ì´í‹°ë¸Œ Metal ìµœì í™” |

### ê²°ê³¼ íŒŒì¼

ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

```
report/
â”œâ”€â”€ benchmark_report_10runs_20250718_151256.md
â”œâ”€â”€ api_benchmark_report_10runs_20250718_151256.md
output/
â”œâ”€â”€ benchmark_results_10runs_20250718_151256.json
â”œâ”€â”€ api_benchmark_results_10runs_20250718_151256.json
logging/
â”œâ”€â”€ benchmark_detailed_20250718_151256.log
â”œâ”€â”€ api_benchmark_detailed_20250718_151256.log
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. PyTorch MPS ì˜¤ë¥˜
```bash
# MPS ë°±ì—”ë“œ í™•ì¸
python -c "import torch; print(torch.backends.mps.is_available())"

# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
```

#### 2. Ollama ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨
```bash
# ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ í›„ ì¬ë“±ë¡
ollama rm gemma-3-1b-it-bench
ollama create gemma-3-1b-it-bench -f ./models/Modelfile
```

#### 3. llama.cpp Metal ë¯¸ì§€ì›
```bash
# Metal ì§€ì› ì¬ë¹Œë“œ
cd llama.cpp
make clean
make GGML_METAL=1
```

#### 4. Uzu ë¹Œë“œ ì˜¤ë¥˜
```bash
# Xcode Command Line Tools ì„¤ì¹˜ í™•ì¸
xcode-select -p
# ì¶œë ¥: /Applications/Xcode.app/Contents/Developer ë˜ëŠ” /Library/Developer/CommandLineTools

# Xcode ë¼ì´ì„¼ìŠ¤ ë™ì˜
sudo xcodebuild -license accept

# Rust íˆ´ì²´ì¸ ì—…ë°ì´íŠ¸
rustup update stable

# Metal ì»´íŒŒì¼ ì˜¤ë¥˜ ì‹œ Xcode ì¬ì„¤ì¹˜
sudo xcode-select --reset
xcode-select --install
```

**ì¼ë°˜ì ì¸ Uzu ë¹Œë“œ ì˜¤ë¥˜:**

1. **Metal ì…°ì´ë” ì»´íŒŒì¼ ì‹¤íŒ¨**
   ```
   error: failed to run custom build command for `uzu`
   xcrun: error: unable to find utility "metal"
   ```
   **í•´ê²°**: `xcode-select --install`ë¡œ Command Line Tools ì„¤ì¹˜

2. **Apple í”„ë ˆì„ì›Œí¬ ë§í¬ ì˜¤ë¥˜**
   ```
   error: linking with `cc` failed: exit status: 1
   ld: framework not found Metal
   ```
   **í•´ê²°**: Xcode ë¼ì´ì„¼ìŠ¤ ë™ì˜ ë° ê°œë°œì ë„êµ¬ í™œì„±í™”

3. **Rust + Metal í˜¸í™˜ì„± ë¬¸ì œ**
   ```
   error: failed to compile `metal` v0.x.x
   ```
   **í•´ê²°**: `rustup update`ë¡œ ìµœì‹  Rust ì‚¬ìš©

### ì„œë²„ í¬íŠ¸ ì¶©ëŒ

ê° ì„œë²„ëŠ” ê³ ìœ í•œ í¬íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:
- PyTorch: 8001
- llama.cpp: 8002  
- Uzu: 8000
- Ollama: 11434 (ê¸°ë³¸ê°’)

## ğŸ“š ì¶”ê°€ ì •ë³´

### ë²¤ì¹˜ë§ˆí¬ ì„¤ì • ê°€ì´ë“œ

ìì„¸í•œ ì„¤ì • ë°©ë²•ì€ `benchmark_usage.md`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
uzu/
â”œâ”€â”€ benchmark_config.json       # í†µí•© ì„¤ì • íŒŒì¼
â”œâ”€â”€ subprocess_benchmark.py     # CLI ë²¤ì¹˜ë§ˆí¬
â”œâ”€â”€ api_benchmark.py           # API ë²¤ì¹˜ë§ˆí¬
â”œâ”€â”€ server_manager.py          # ì„œë²„ ê´€ë¦¬
â”œâ”€â”€ pytorch_server.py          # PyTorch ì„œë²„
â”œâ”€â”€ benchmark_prompts.py       # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ
â”œâ”€â”€ benchmark_usage.md         # ìƒì„¸ ì‚¬ìš©ë²• ê°€ì´ë“œ
â”œâ”€â”€ models/                    # ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ gemma-3-1b-it/        # HuggingFace ì›ë³¸
â”‚   â”œâ”€â”€ gemma-3-1b-it-uzu/    # Uzu í˜•ì‹
â”‚   â”œâ”€â”€ gemma-3-1b-it-gguf-llama/ # llama.cpp GGUF
â”‚   â””â”€â”€ Modelfile             # Ollama ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ report/                   # Markdown ë¦¬í¬íŠ¸
â”œâ”€â”€ output/                   # JSON ê²°ê³¼ ë°ì´í„°
â””â”€â”€ logging/                  # ìƒì„¸ ë¡œê·¸
```

### í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ì„¤ê³„ ì² í•™

ë²¤ì¹˜ë§ˆí¬ì˜ í•µì‹¬ì€ **ê³µì •í•œ ë¹„êµ**ì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì„¤ê³„ ì›ì¹™ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤:

1. **ë™ì¼í•œ ì…ë ¥**: ëª¨ë“  ì—”ì§„ì´ ë™ì¼í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ìŠµë‹ˆë‹¤
2. **ì¼ê´€ëœ ì œì•½**: í† í° ìˆ˜, ì˜¨ë„, ì–¸ì–´ ë“± ëª¨ë“  ìƒì„± ì¡°ê±´ì„ í†µì¼í–ˆìŠµë‹ˆë‹¤  
3. **êµ¬ì¡°í™”ëœ ì‘ë‹µ**: 300ì ì œí•œê³¼ 3ë‹¨ê³„ êµ¬ì¡°ë¡œ ì‘ë‹µ í’ˆì§ˆì„ í‘œì¤€í™”í–ˆìŠµë‹ˆë‹¤
4. **ë‹¤ì–‘í•œ ë³µì¡ë„**: ì§§ì€/ì¤‘ê°„/ê¸´ í”„ë¡¬í”„íŠ¸ë¡œ ë‹¤ì–‘í•œ ì¶”ë¡  ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤
5. **ì¬í˜„ ê°€ëŠ¥ì„±**: ëª¨ë“  í”„ë¡¬í”„íŠ¸ì™€ ì„¤ì •ì´ ì½”ë“œë¡œ ê´€ë¦¬ë˜ì–´ ì¬í˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤

### ê¸°ì—¬í•˜ê¸°

ì´ìŠˆ ì‹ ê³ ë‚˜ ê°œì„  ì œì•ˆì€ GitHub Issuesë¥¼ í†µí•´ ì œì¶œí•´ì£¼ì„¸ìš”.

### ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-07-18 15:12:56  
**í…ŒìŠ¤íŠ¸ í™˜ê²½**: macOS 15.5, Apple M3 Max, 36GB RAM 