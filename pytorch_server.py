#!/usr/bin/env python3
"""
PyTorch Gemma λ¨λΈμ„ FastAPI μ„λ²„λ΅ μ κ³µ
OpenAI νΈν™ API ν•νƒλ΅ /chat/completions μ—”λ“ν¬μΈνΈ μ κ³µ
"""

import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import os
import json

# HuggingFace tokenizers λ³‘λ ¬μ²λ¦¬ κ²½κ³  μ–µμ 
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

app = FastAPI(title="PyTorch Gemma Server", version="1.0.0")

# μ „μ—­ λ¨λΈ λ° ν† ν¬λ‚μ΄μ €
model = None
tokenizer = None

class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: Optional[int] = 500
    temperature: Optional[float] = 0.3
    stream: Optional[bool] = False

class ChatCompletionResponse(BaseModel):
    id: str
    object: str
    created: int
    model: str
    choices: List[dict]
    usage: dict

def load_model(model_path: str = "./models/gemma-3-1b-it"):
    """λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ €λ¥Ό λ΅λ”©"""
    global model, tokenizer
    
    print(f"π”¥ PyTorch λ¨λΈ λ΅λ”© μ‹μ‘: {model_path}")
    start_time = time.time()
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="mps"
    )
    
    load_time = time.time() - start_time
    print(f"β… PyTorch λ¨λΈ λ΅λ”© μ™„λ£: {load_time:.2f}μ΄")

@app.on_event("startup")
async def startup_event():
    """μ„λ²„ μ‹μ‘ μ‹ λ¨λΈ λ΅λ”©"""
    model_path = os.getenv("PYTORCH_MODEL_PATH", "./models/gemma-3-1b-it")
    load_model(model_path)

@app.get("/")
async def health_check():
    """ν—¬μ¤μ²΄ν¬ μ—”λ“ν¬μΈνΈ"""
    return {"status": "ok", "model_loaded": model is not None}

@app.post("/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI νΈν™ μ±„ν… μ™„μ„± API"""
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # λ©”μ‹μ§€λ¥Ό ν”„λ΅¬ν”„νΈλ΅ λ³€ν™
        formatted_prompt = tokenizer.apply_chat_template(
            [{"role": msg.role, "content": msg.content} for msg in request.messages],
            tokenize=False,
            add_generation_prompt=True
        )
        
        # ν† ν°ν™”
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to("mps")
        
        # μƒμ„±
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                do_sample=True,
                temperature=request.temperature,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # μƒμ„±λ ν† ν°λ§ λ””μ½”λ”©
        input_length = inputs['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        response_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        generation_time = time.time() - start_time
        
        # OpenAI νΈν™ μ‘λ‹µ ν•νƒ
        response = ChatCompletionResponse(
            id=f"chatcmpl-{int(time.time())}",
            object="chat.completion",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": input_length,
                "completion_tokens": len(generated_tokens),
                "total_tokens": input_length + len(generated_tokens),
                "generation_time": generation_time
            }
        )
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="PyTorch Gemma Server")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8001, help="Port to bind to")
    parser.add_argument("--model-path", default="./models/gemma-3-1b-it", help="Model path")
    
    args = parser.parse_args()
    
    # ν™κ²½λ³€μ μ„¤μ •
    os.environ["PYTORCH_MODEL_PATH"] = args.model_path
    
    print(f"π€ PyTorch μ„λ²„ μ‹μ‘: {args.host}:{args.port}")
    print(f"π“ λ¨λΈ κ²½λ΅: {args.model_path}")
    
    uvicorn.run(app, host=args.host, port=args.port, log_level="info") 